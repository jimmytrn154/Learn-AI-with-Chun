{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c49cffb",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e68bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 280 annotated frames to viz/Backpack_0\n"
     ]
    }
   ],
   "source": [
    "import json, cv2, os\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "video_path = \"train/samples/Backpack_0/drone_video.mp4\"\n",
    "pred_json  = \"predictions.json\"         # your submission-style file\n",
    "gt_json    = \"train/annotations/annotations.json\"  # optional; set to None if not available\n",
    "video_id   = \"Backpack_0\"          # must match keys in JSON\n",
    "out_dir    = \"viz/Backpack_0\"\n",
    "write_video = True                       # also write an MP4 with overlays\n",
    "fps_out     = 25                         # match input fps if you know it\n",
    "# ----------------------------\n",
    "\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def index_bboxes(entries, key=\"detections\"):\n",
    "    \"\"\"Return dict: frame -> list of boxes for a given video_id.\"\"\"\n",
    "    by_vid = {e[\"video_id\"]: e for e in entries}\n",
    "    if video_id not in by_vid: return {}\n",
    "    recs = by_vid[video_id].get(key, [])\n",
    "    if not recs: return {}\n",
    "    # schema has a list with one item holding \"bboxes\"\n",
    "    if isinstance(recs, list) and len(recs)>0 and \"bboxes\" in recs[0]:\n",
    "        bbs = recs[0][\"bboxes\"]\n",
    "    else:\n",
    "        # fallback to direct bboxes list if your tooling stores it that way\n",
    "        bbs = recs\n",
    "    out = {}\n",
    "    for b in bbs:\n",
    "        k = int(b[\"frame\"])\n",
    "        out.setdefault(k, []).append((int(b[\"x1\"]), int(b[\"y1\"]), int(b[\"x2\"]), int(b[\"y2\"])))\n",
    "    return out\n",
    "\n",
    "def load_json(path):\n",
    "    if path is None or not os.path.exists(path): return None\n",
    "    with open(path, \"r\") as f: return json.load(f)\n",
    "\n",
    "pred = load_json(pred_json) or []\n",
    "gt   = load_json(gt_json) or []\n",
    "\n",
    "pred_idx = index_bboxes(pred, key=\"detections\")\n",
    "gt_idx   = index_bboxes(gt,   key=\"annotations\")\n",
    "\n",
    "def iou(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "    ix1, iy1 = max(ax1,bx1), max(ay1,by1)\n",
    "    ix2, iy2 = min(ax2,bx2), min(ay2,by2)\n",
    "    iw, ih = max(0, ix2-ix1), max(0, iy2-iy1)\n",
    "    inter = iw*ih\n",
    "    if inter == 0: return 0.0\n",
    "    area_a = (ax2-ax1)*(ay2-ay1)\n",
    "    area_b = (bx2-bx1)*(by2-by1)\n",
    "    return inter / float(area_a + area_b - inter + 1e-6)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Cannot open video: {video_path}\")\n",
    "\n",
    "# prepare writer if needed\n",
    "writer = None\n",
    "if write_video:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or fps_out\n",
    "    writer = cv2.VideoWriter(str(Path(out_dir, \"overlay.mp4\")), fourcc, fps, (w, h))\n",
    "\n",
    "frame_id = 0\n",
    "saved = 0\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "\n",
    "    preds = pred_idx.get(frame_id, [])\n",
    "    gts   = gt_idx.get(frame_id, [])\n",
    "\n",
    "    if preds or gts:\n",
    "        vis = frame.copy()\n",
    "\n",
    "        # draw GT in RED\n",
    "        for (x1,y1,x2,y2) in gts:\n",
    "            cv2.rectangle(vis, (x1,y1), (x2,y2), (0,0,255), 2)\n",
    "            cv2.putText(vis, \"GT\", (x1, max(0,y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # draw predictions in GREEN and show IoU of best match (if any GT)\n",
    "        for (x1,y1,x2,y2) in preds:\n",
    "            cv2.rectangle(vis, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "            label = \"PRED\"\n",
    "            if gts:\n",
    "                best = max(iou((x1,y1,x2,y2), g) for g in gts)\n",
    "                label += f\" IoU={best:.2f}\"\n",
    "            cv2.putText(vis, label, (x1, min(vis.shape[0]-4, y2+18)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # add legend\n",
    "        cv2.putText(vis, \"Legend: PRED=Green, GT=Red\", (10, 24),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(vis, f\"frame {frame_id}\", (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # save PNG of this frame\n",
    "        out_path = Path(out_dir, f\"frame_{frame_id:06d}.png\")\n",
    "        cv2.imwrite(str(out_path), vis)\n",
    "        saved += 1\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.write(vis)\n",
    "    else:\n",
    "        # still write to video if you want a full overlay stream\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "\n",
    "    frame_id += 1\n",
    "\n",
    "cap.release()\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "print(f\"Saved {saved} annotated frames to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69d72e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1454 annotated frames to viz/Backpack_1\n",
      "Wrote video: viz\\Backpack_1\\overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "# viz_overlays.py\n",
    "# Visualize predictions vs ground-truth for the drone instance-localization task.\n",
    "# - Handles MULTIPLE GT segments per video_id (annotations: [ {\"bboxes\":[...]}, {\"bboxes\":[...]} , ... ])\n",
    "# - Accepts predictions in either flat or segmented format:\n",
    "#     {\"video_id\": \"...\", \"detections\": [{\"bboxes\":[...]}, {\"bboxes\":[...]}]}\n",
    "#     or {\"video_id\": \"...\", \"detections\": {\"bboxes\":[...]}}\n",
    "#     or {\"video_id\": \"...\", \"detections\": [...] }  # where the list is bboxes directly\n",
    "#\n",
    "# Output: per-frame PNGs (only for frames with any GT/PRED box) and an overlay MP4 (optional).\n",
    "\n",
    "import json, os, cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG — edit these paths\n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"train/samples/Backpack_1/drone_video.mp4\"\n",
    "VIDEO_ID   = \"Backpack_1\"\n",
    "\n",
    "GT_JSON    = \"train/annotations/annotations.json\"   # set to None to skip GT\n",
    "PRED_JSON  = \"predictions.json\"               # set to None to skip predictions\n",
    "\n",
    "OUT_DIR        = f\"viz/{VIDEO_ID}\"\n",
    "WRITE_VIDEO    = True\n",
    "VIDEO_FILENAME = \"overlay.mp4\"\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def safe_load_json(path):\n",
    "    if path is None or not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _extract_segments(record, key):\n",
    "    \"\"\"\n",
    "    record[key] might be:\n",
    "      - a list of segments [{ \"bboxes\": [...] }, {\"bboxes\": [...]}, ...]\n",
    "      - a single dict with \"bboxes\"\n",
    "      - a flat list of dicts that are actually bboxes\n",
    "    Return a list of segments, each as {\"bboxes\": [ {frame,x1,y1,x2,y2}, ... ]}.\n",
    "    \"\"\"\n",
    "    segs = record.get(key, [])\n",
    "    # normalize\n",
    "    if isinstance(segs, dict) and \"bboxes\" in segs:\n",
    "        return [ {\"bboxes\": segs[\"bboxes\"]} ]\n",
    "    if isinstance(segs, list):\n",
    "        # Could be list of segments OR list of bbox dicts\n",
    "        if len(segs) > 0 and isinstance(segs[0], dict) and \"bboxes\" in segs[0]:\n",
    "            # already list of segments\n",
    "            return segs\n",
    "        elif len(segs) > 0 and isinstance(segs[0], dict) and \"frame\" in segs[0]:\n",
    "            # it's a flat list of bbox dicts\n",
    "            return [ {\"bboxes\": segs} ]\n",
    "        else:\n",
    "            # empty or unknown — treat as no segments\n",
    "            return []\n",
    "    # anything else → no segments\n",
    "    return []\n",
    "\n",
    "\n",
    "def index_bboxes_all_segments(entries, video_id, key=\"annotations\"):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      - frame_to_boxes: { frame_idx: [(x1,y1,x2,y2), ...] }\n",
    "      - segments: [ {\"bboxes\":[ {...}, {...}, ... ]}, ... ]\n",
    "    Works for both GT (\"annotations\") and predictions (\"detections\").\n",
    "    \"\"\"\n",
    "    if not entries:\n",
    "        return {}, []\n",
    "    by_vid = {e.get(\"video_id\"): e for e in entries if \"video_id\" in e}\n",
    "    if video_id not in by_vid:\n",
    "        return {}, []\n",
    "    rec = by_vid[video_id]\n",
    "\n",
    "    segments = _extract_segments(rec, key)\n",
    "    frame_to_boxes = {}\n",
    "\n",
    "    for seg in segments:\n",
    "        bbs = seg.get(\"bboxes\", [])\n",
    "        for b in bbs:\n",
    "            k  = int(b[\"frame\"])\n",
    "            x1 = int(b[\"x1\"]); y1 = int(b[\"y1\"]); x2 = int(b[\"x2\"]); y2 = int(b[\"y2\"])\n",
    "            frame_to_boxes.setdefault(k, []).append((x1, y1, x2, y2))\n",
    "\n",
    "    return frame_to_boxes, segments\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    xx1, yy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    xx2, yy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    w, h = max(0, xx2 - xx1), max(0, yy2 - yy1)\n",
    "    inter = w * h\n",
    "    if inter <= 0:\n",
    "        return 0.0\n",
    "    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n",
    "    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n",
    "    return float(inter) / float(area_a + area_b - inter + 1e-6)\n",
    "\n",
    "\n",
    "def main():\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gt_entries   = safe_load_json(GT_JSON) or []\n",
    "    pred_entries = safe_load_json(PRED_JSON) or []\n",
    "\n",
    "    gt_frame2boxes, _   = index_bboxes_all_segments(gt_entries,   VIDEO_ID, key=\"annotations\")\n",
    "    pred_frame2boxes, _ = index_bboxes_all_segments(pred_entries, VIDEO_ID, key=\"detections\")\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "\n",
    "    writer = None\n",
    "    if WRITE_VIDEO:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(str(Path(OUT_DIR, VIDEO_FILENAME)), fourcc, fps, (w, h))\n",
    "\n",
    "    frame_id = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        gts   = gt_frame2boxes.get(frame_id, [])\n",
    "        preds = pred_frame2boxes.get(frame_id, [])\n",
    "\n",
    "        if gts or preds:\n",
    "            vis = frame.copy()\n",
    "\n",
    "            # Draw GT (RED)\n",
    "            for (x1, y1, x2, y2) in gts:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(vis, \"GT\", (x1, max(0, y1 - 6)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Draw predictions (GREEN) and best IoU vs any GT on this frame\n",
    "            for (x1, y1, x2, y2) in preds:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label = \"PRED\"\n",
    "                if gts:\n",
    "                    best_iou = max(iou((x1, y1, x2, y2), g) for g in gts)\n",
    "                    label += f\" IoU={best_iou:.2f}\"\n",
    "                cv2.putText(vis, label, (x1, min(h - 6, y2 + 18)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Header text\n",
    "            legend = \"Legend: PRED=Green, GT=Red\"\n",
    "            cv2.putText(vis, legend, (10, 26),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(vis, f\"video_id={VIDEO_ID}  frame={frame_id}\",\n",
    "                        (10, 52), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Save this annotated frame as PNG\n",
    "            out_png = Path(OUT_DIR, f\"frame_{frame_id:06d}.png\")\n",
    "            cv2.imwrite(str(out_png), vis)\n",
    "            saved += 1\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.write(vis)\n",
    "        else:\n",
    "            # If you want a full-length overlay video, write the raw frame too\n",
    "            if writer is not None:\n",
    "                writer.write(frame)\n",
    "\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print(f\"Saved {saved} annotated frames to {OUT_DIR}\")\n",
    "    if WRITE_VIDEO:\n",
    "        print(f\"Wrote video: {Path(OUT_DIR, VIDEO_FILENAME)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab105d99",
   "metadata": {},
   "source": [
    "## Main baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "baseline_predictor.py\n",
    "Baseline: tiny YOLO (class-agnostic proposals) + ResNet18 embedding matcher + IoU/appearance tracking\n",
    "Outputs submission-style predictions.json and (optionally) ST-IoU on a validation split.\n",
    "\n",
    "Directory layout expected (per your spec):\n",
    "dataset/\n",
    " ├─ samples/\n",
    " │   ├─ drone_video_001/\n",
    " │   │   ├─ object_images/ img_1.jpg img_2.jpg img_3.jpg\n",
    " │   │   └─ drone_video.mp4\n",
    " │   ├─ drone_video_002/ ...\n",
    " │   └─ ...\n",
    " └─ annotations/ annotations.json          # optional (only for eval / viz)\n",
    "\n",
    "Run:\n",
    "python baseline_predictor.py --data_root dataset --out predictions.json\n",
    "Add --eval to compute ST-IoU against annotations/annotations.json\n",
    "\"\"\"\n",
    "\n",
    "import os, json, math, argparse, random, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Optional (tiny) YOLO proposals\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    _HAS_YOLO = True\n",
    "except Exception:\n",
    "    _HAS_YOLO = False\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Utility\n",
    "# ------------------------------\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def to_device(x):\n",
    "    return x.cuda() if torch.cuda.is_available() else x\n",
    "\n",
    "def l2_normalize(x, dim=-1, eps=1e-6):\n",
    "    return x / (x.norm(dim=dim, keepdim=True) + eps)\n",
    "\n",
    "def iou_xyxy(a, b) -> float:\n",
    "    ax1, ay1, ax2, ay2 = a; bx1, by1, bx2, by2 = b\n",
    "    xx1, yy1 = max(ax1,bx1), max(ay1,by1)\n",
    "    xx2, yy2 = min(ax2,bx2), min(ay2,by2)\n",
    "    w, h = max(0, xx2-xx1), max(0, yy2-yy1)\n",
    "    inter = w*h\n",
    "    if inter == 0: return 0.0\n",
    "    area_a = max(0, ax2-ax1) * max(0, ay2-ay1)\n",
    "    area_b = max(0, bx2-bx1) * max(0, by2-by1)\n",
    "    return inter / float(area_a + area_b - inter + 1e-6)\n",
    "\n",
    "def nms_xyxy(boxes, scores, iou_thr=0.5):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    boxes = np.asarray(boxes, dtype=float)\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    x1, y1, x2, y2 = boxes.T\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n",
    "        inds = np.where(ovr <= iou_thr)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Data helpers\n",
    "# ------------------------------\n",
    "def find_episodes(data_root: str) -> List[str]:\n",
    "    \"\"\"Return sorted list of video_id directories under samples/\"\"\"\n",
    "    p = Path(data_root) / \"samples\"\n",
    "    vids = [d.name for d in p.iterdir() if d.is_dir()]\n",
    "    vids.sort()\n",
    "    return vids\n",
    "\n",
    "def load_refs_for_episode(data_root: str, video_id: str) -> List[np.ndarray]:\n",
    "    ref_dir = Path(data_root) / \"samples\" / video_id / \"object_images\"\n",
    "    imgs = []\n",
    "    for p in sorted(ref_dir.glob(\"*\")):\n",
    "        if p.suffix.lower() in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "            im = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "            if im is not None:\n",
    "                imgs.append(im)\n",
    "    if len(imgs) == 0:\n",
    "        raise RuntimeError(f\"No reference images for {video_id}\")\n",
    "    return imgs[:3]\n",
    "\n",
    "def video_path_for_episode(data_root: str, video_id: str) -> str:\n",
    "    return str(Path(data_root) / \"samples\" / video_id / \"drone_video.mp4\")\n",
    "\n",
    "def load_annotations_json(data_root: str) -> list:\n",
    "    p = Path(data_root) / \"annotations\" / \"annotations.json\"\n",
    "    if p.exists():\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def frame_to_boxes(entries: list, video_id: str, key: str) -> Dict[int, List[Tuple[int,int,int,int]]]:\n",
    "    \"\"\"Handle multi-segment annotations/detections.\"\"\"\n",
    "    if not entries: return {}\n",
    "    by_vid = {e.get(\"video_id\"): e for e in entries if \"video_id\" in e}\n",
    "    if video_id not in by_vid: return {}\n",
    "    rec = by_vid[video_id]\n",
    "    segs = rec.get(key, [])\n",
    "    # normalize shapes\n",
    "    if isinstance(segs, dict) and \"bboxes\" in segs:\n",
    "        segs = [segs]\n",
    "    if isinstance(segs, list) and segs and \"frame\" in segs[0]:\n",
    "        segs = [{\"bboxes\": segs}]\n",
    "    out = {}\n",
    "    for seg in segs:\n",
    "        for b in seg.get(\"bboxes\", []):\n",
    "            k = int(b[\"frame\"])\n",
    "            out.setdefault(k, []).append((int(b[\"x1\"]), int(b[\"y1\"]), int(b[\"x2\"]), int(b[\"y2\"])))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Proposal generator (YOLO nano)\n",
    "# ------------------------------\n",
    "class YOLOProposals:\n",
    "    \"\"\"Class-agnostic proposals using a tiny YOLO; if YOLO unavailable, falls back to sliding windows.\"\"\"\n",
    "    def __init__(self, conf=0.05, iou=0.7, imgsz=640, max_candidates=200):\n",
    "        self.conf = conf; self.iou = iou; self.imgsz = imgsz; self.max_candidates = max_candidates\n",
    "        self.model = None\n",
    "        if _HAS_YOLO:\n",
    "            try:\n",
    "                # yolov8n.pt is ~3.2M params\n",
    "                self.model = YOLO(\"yolov8n.pt\")\n",
    "            except Exception:\n",
    "                self.model = None\n",
    "\n",
    "    def __call__(self, frame_bgr: np.ndarray) -> List[Tuple[int,int,int,int]]:\n",
    "        H, W = frame_bgr.shape[:2]\n",
    "        if self.model is None:\n",
    "            # simple sliding window fallback (sparse to stay cheap)\n",
    "            sizes = [64, 96, 128, 160, 192]\n",
    "            stride = 0.5  # 50% overlap\n",
    "            props = []\n",
    "            for s in sizes:\n",
    "                sx = max(16, int(s))\n",
    "                sy = max(16, int(s))\n",
    "                step_x = max(8, int(sx * (1 - stride)))\n",
    "                step_y = max(8, int(sy * (1 - stride)))\n",
    "                for y in range(0, H - sy, step_y):\n",
    "                    for x in range(0, W - sx, step_x):\n",
    "                        props.append((x, y, x + sx, y + sy))\n",
    "                        if len(props) >= self.max_candidates:\n",
    "                            return props\n",
    "            return props[: self.max_candidates]\n",
    "        # YOLO forward\n",
    "        res = self.model.predict(frame_bgr, conf=self.conf, iou=self.iou, imgsz=self.imgsz, verbose=False)[0]\n",
    "        boxes = res.boxes.xyxy.cpu().numpy() if res.boxes is not None else np.zeros((0,4))\n",
    "        # class-agnostic: ignore cls; cap candidates\n",
    "        boxes = boxes[: self.max_candidates]\n",
    "        out = []\n",
    "        for x1,y1,x2,y2 in boxes:\n",
    "            x1 = int(max(0, min(W-1, x1))); y1 = int(max(0, min(H-1, y1)))\n",
    "            x2 = int(max(0, min(W-1, x2))); y2 = int(max(0, min(H-1, y2)))\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                out.append((x1,y1,x2,y2))\n",
    "        return out\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Embedding model (ResNet-18 + tiny head)\n",
    "# ------------------------------\n",
    "class EmbeddingMatcher(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet-18 backbone (frozen) + projection head 512->256 with L2 normalize.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim=256):\n",
    "        super().__init__()\n",
    "        rn18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        for p in rn18.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        # take features before FC\n",
    "        self.backbone = nn.Sequential(*list(rn18.children())[:-1])  # (B,512,1,1)\n",
    "        self.head = nn.Linear(512, out_dim, bias=False)\n",
    "        self.prep = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                 std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_np(self, np_imgs: List[np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"np_imgs: list of HxWxC BGR or RGB? We'll convert BGR->RGB.\"\"\"\n",
    "        if len(np_imgs) == 0:\n",
    "            return torch.empty(0, 256)\n",
    "        tensors = []\n",
    "        for img in np_imgs:\n",
    "            if img.shape[-1] == 3:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            tensors.append(self.prep(img))\n",
    "        batch = torch.stack(tensors, dim=0)\n",
    "        batch = to_device(batch).float()\n",
    "        feats = self.backbone(batch).flatten(1)         # (B,512)\n",
    "        proj  = self.head(feats)                        # (B,256)\n",
    "        proj  = l2_normalize(proj, dim=1)\n",
    "        return proj\n",
    "\n",
    "def build_template(matcher: EmbeddingMatcher, ref_imgs: List[np.ndarray], augs_per_ref=16) -> torch.Tensor:\n",
    "    \"\"\"Augment refs, embed, and average → template vector (1, D).\"\"\"\n",
    "    # Simple augmentation: random rescale + color jitter + small blur\n",
    "    auged = []\n",
    "    for im in ref_imgs:\n",
    "        h,w = im.shape[:2]\n",
    "        for _ in range(augs_per_ref):\n",
    "            scale = np.random.uniform(0.8, 1.2)\n",
    "            nh, nw = max(16, int(h*scale)), max(16, int(w*scale))\n",
    "            resized = cv2.resize(im, (nw, nh), interpolation=cv2.INTER_AREA if scale<1 else cv2.INTER_LINEAR)\n",
    "            if np.random.rand() < 0.3:\n",
    "                k = np.random.choice([3,5])\n",
    "                resized = cv2.GaussianBlur(resized, (k,k), 0)\n",
    "            auged.append(resized)\n",
    "    with torch.no_grad():\n",
    "        embs = matcher.encode_np(auged)  # (N, D)\n",
    "        tmpl = embs.mean(0, keepdim=True)  # (1, D)\n",
    "        tmpl = l2_normalize(tmpl, dim=1)\n",
    "    return tmpl  # (1, D)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Segmenter / Tracker logic\n",
    "# ------------------------------\n",
    "class SingleTargetTracker:\n",
    "    \"\"\"\n",
    "    Keeps a single track using IoU + appearance similarity.\n",
    "    Hysteresis thresholds for start/keep; min segment length & gap fill.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tau_high=0.45, tau_low=0.35,\n",
    "                 assoc_lambda=0.5,\n",
    "                 search_roi_pad=0.35,\n",
    "                 max_lost=10,\n",
    "                 min_commit=3,\n",
    "                 gap_fill=2):\n",
    "        self.tau_high = tau_high\n",
    "        self.tau_low  = tau_low\n",
    "        self.assoc_lambda = assoc_lambda\n",
    "        self.search_roi_pad = search_roi_pad\n",
    "        self.max_lost = max_lost\n",
    "        self.min_commit = min_commit\n",
    "        self.gap_fill = gap_fill\n",
    "\n",
    "        self.active = False\n",
    "        self.last_box = None\n",
    "        self.lost = 0\n",
    "        self.buffer = []     # pending boxes before commit\n",
    "        self.current_segment = []  # committed boxes for the ongoing segment\n",
    "        self.detections = []  # committed for entire video (flat list)\n",
    "        self.prev_frame = None\n",
    "\n",
    "    def _search_window(self, frame_shape):\n",
    "        H,W = frame_shape[:2]\n",
    "        if self.last_box is None:\n",
    "            return (0,0,W-1,H-1)\n",
    "        x1,y1,x2,y2 = self.last_box\n",
    "        pad = int(self.search_roi_pad * max(x2-x1, y2-y1))\n",
    "        rx1 = max(0, x1 - pad); ry1 = max(0, y1 - pad)\n",
    "        rx2 = min(W-1, x2 + pad); ry2 = min(H-1, y2 + pad)\n",
    "        return (rx1,ry1,rx2,ry2)\n",
    "\n",
    "    def _associate(self, boxes, sims):\n",
    "        if not boxes:\n",
    "            return None, None\n",
    "        if self.last_box is None or self.lost >= self.max_lost:\n",
    "            # pick best by similarity\n",
    "            idx = int(np.argmax(sims))\n",
    "            return boxes[idx], sims[idx]\n",
    "        best_val = -1e9; best_idx = -1\n",
    "        for i,(b,s) in enumerate(zip(boxes, sims)):\n",
    "            val = self.assoc_lambda * iou_xyxy(b, self.last_box) + (1.0 - self.assoc_lambda) * float(s)\n",
    "            if val > best_val:\n",
    "                best_val = val; best_idx = i\n",
    "        return boxes[best_idx], sims[best_idx]\n",
    "\n",
    "    def _commit_buffer_if_ready(self):\n",
    "        if len(self.buffer) >= self.min_commit:\n",
    "            self.current_segment.extend(self.buffer)\n",
    "            self.detections.extend(self.buffer)\n",
    "            self.buffer = []\n",
    "            self.active = True\n",
    "\n",
    "    def _flush_segment(self):\n",
    "        self.buffer = []\n",
    "        self.current_segment = []\n",
    "        self.active = False\n",
    "\n",
    "    def update(self, frame_idx: int, frame_shape, candidate_boxes: List[Tuple[int,int,int,int]], candidate_scores: List[float]):\n",
    "        # select one candidate (or none)\n",
    "        chosen, sim = self._associate(candidate_boxes, candidate_scores) if candidate_boxes else (None, None)\n",
    "\n",
    "        # gating with hysteresis\n",
    "        if chosen is not None:\n",
    "            if not self.active:\n",
    "                # pre-commit buffer with tau_high\n",
    "                if sim >= self.tau_high:\n",
    "                    self.buffer.append({\"frame\": int(frame_idx), \"x1\": int(chosen[0]), \"y1\": int(chosen[1]), \"x2\": int(chosen[2]), \"y2\": int(chosen[3])})\n",
    "                    self._commit_buffer_if_ready()\n",
    "            else:\n",
    "                # keep segment with tau_low\n",
    "                if sim >= self.tau_low:\n",
    "                    # fill small gaps if any\n",
    "                    if self.prev_frame is not None and frame_idx - self.prev_frame > 1 and (frame_idx - self.prev_frame - 1) <= self.gap_fill and len(self.current_segment)>0:\n",
    "                        # simple linear interp of boxes\n",
    "                        last = self.current_segment[-1]\n",
    "                        num_gap = frame_idx - self.prev_frame - 1\n",
    "                        for g in range(1, num_gap+1):\n",
    "                            t = g / (num_gap+1)\n",
    "                            interp = (\n",
    "                                int((1-t)*last[\"x1\"] + t*chosen[0]),\n",
    "                                int((1-t)*last[\"y1\"] + t*chosen[1]),\n",
    "                                int((1-t)*last[\"x2\"] + t*chosen[2]),\n",
    "                                int((1-t)*last[\"y2\"] + t*chosen[3]),\n",
    "                            )\n",
    "                            self.current_segment.append({\"frame\": int(self.prev_frame+g), \"x1\": interp[0], \"y1\": interp[1], \"x2\": interp[2], \"y2\": interp[3]})\n",
    "                            self.detections.append(self.current_segment[-1])\n",
    "                    self.current_segment.append({\"frame\": int(frame_idx), \"x1\": int(chosen[0]), \"y1\": int(chosen[1]), \"x2\": int(chosen[2]), \"y2\": int(chosen[3])})\n",
    "                    self.detections.append(self.current_segment[-1])\n",
    "                else:\n",
    "                    # similarity too low -> close the segment\n",
    "                    self._flush_segment()\n",
    "\n",
    "            self.last_box = chosen\n",
    "            self.lost = 0\n",
    "        else:\n",
    "            # no candidate\n",
    "            self.lost += 1\n",
    "            if self.lost >= self.max_lost:\n",
    "                self._flush_segment()\n",
    "\n",
    "        self.prev_frame = frame_idx\n",
    "\n",
    "    def finalize(self) -> List[dict]:\n",
    "        # If we never committed (buffer only), drop it\n",
    "        self.buffer = []\n",
    "        return self.detections\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ST-IoU evaluator\n",
    "# ------------------------------\n",
    "def _extract_segments(record, key):\n",
    "    segs = record.get(key, [])\n",
    "    if isinstance(segs, dict) and \"bboxes\" in segs: return [segs]\n",
    "    if isinstance(segs, list) and segs and \"frame\" in segs[0]: return [{\"bboxes\": segs}]\n",
    "    return segs if isinstance(segs, list) else []\n",
    "\n",
    "def st_iou_one(gt_entries, pred_entries, video_id: str) -> float:\n",
    "    # frame -> list of boxes\n",
    "    gt_map   = frame_to_boxes(gt_entries,   video_id, key=\"annotations\")\n",
    "    pred_map = frame_to_boxes(pred_entries, video_id, key=\"detections\")\n",
    "    inter = sorted(set(gt_map.keys()).intersection(pred_map.keys()))\n",
    "    union = sorted(set(gt_map.keys()).union(pred_map.keys()))\n",
    "    if not union: return 0.0\n",
    "    num = 0.0\n",
    "    for f in inter:\n",
    "        # single-target: credit best match\n",
    "        best = 0.0\n",
    "        for g in gt_map[f]:\n",
    "            for p in pred_map[f]:\n",
    "                best = max(best, iou_xyxy(g,p))\n",
    "        num += best\n",
    "    den = float(len(union))  # sum of 1 over union frames\n",
    "    return num / den\n",
    "\n",
    "def st_iou_mean(gt_entries, pred_entries, video_ids: Iterable[str]) -> float:\n",
    "    vals = [st_iou_one(gt_entries, pred_entries, vid) for vid in video_ids]\n",
    "    return float(np.mean(vals)) if vals else 0.0\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Main predictor\n",
    "# ------------------------------\n",
    "class Predictor:\n",
    "    def __init__(self,\n",
    "                 conf=0.05, nms_iou=0.7, imgsz=640, max_props=200,\n",
    "                 tau_high=0.45, tau_low=0.35, assoc_lambda=0.5,\n",
    "                 roi_pad=0.35, max_lost=10, min_commit=3, gap_fill=2,\n",
    "                 frame_stride=1, nms_final_iou=0.55):\n",
    "        self.props = YOLOProposals(conf=conf, iou=nms_iou, imgsz=imgsz, max_candidates=max_props)\n",
    "        self.matcher = to_device(EmbeddingMatcher(out_dim=256).eval())\n",
    "        self.tau_high = tau_high; self.tau_low = tau_low\n",
    "        self.assoc_lambda = assoc_lambda\n",
    "        self.roi_pad = roi_pad; self.max_lost = max_lost\n",
    "        self.min_commit = min_commit; self.gap_fill = gap_fill\n",
    "        self.frame_stride = frame_stride\n",
    "        self.nms_final_iou = nms_final_iou\n",
    "\n",
    "        self.crop_prep = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                                 std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _embed_crops(self, crops: List[np.ndarray]) -> torch.Tensor:\n",
    "        tensors = []\n",
    "        for c in crops:\n",
    "            if c is None or c.size == 0: continue\n",
    "            img = cv2.cvtColor(c, cv2.COLOR_BGR2RGB)\n",
    "            tensors.append(self.crop_prep(img))\n",
    "        if not tensors:\n",
    "            return torch.empty(0, 256)\n",
    "        batch = torch.stack(tensors, dim=0)\n",
    "        batch = to_device(batch).float()\n",
    "        feats = self.matcher.backbone(batch).flatten(1)\n",
    "        proj  = l2_normalize(self.matcher.head(feats), dim=1)\n",
    "        return proj\n",
    "\n",
    "    def _cosine_scores(self, embs: torch.Tensor, tmpl: torch.Tensor) -> np.ndarray:\n",
    "        if embs.numel() == 0:\n",
    "            return np.zeros((0,), dtype=np.float32)\n",
    "        sims = F.linear(embs, tmpl)  # (N,1)\n",
    "        return sims.squeeze(-1).detach().float().cpu().numpy()\n",
    "\n",
    "    def _segmentize(self, dets: List[dict], max_gap=1) -> List[dict]:\n",
    "        \"\"\"Split flat detections into segments by frame gaps > max_gap.\"\"\"\n",
    "        dets = sorted(dets, key=lambda d: d[\"frame\"])\n",
    "        segments, cur = [], []\n",
    "        prev = None\n",
    "        for d in dets:\n",
    "            if prev is not None and d[\"frame\"] - prev[\"frame\"] > max_gap:\n",
    "                if cur: segments.append({\"bboxes\": cur}); cur = []\n",
    "            cur.append(d)\n",
    "            prev = d\n",
    "        if cur: segments.append({\"bboxes\": cur})\n",
    "        return segments\n",
    "\n",
    "    def run_episode(self, data_root: str, video_id: str) -> dict:\n",
    "        # build template\n",
    "        refs = load_refs_for_episode(data_root, video_id)\n",
    "        template = build_template(self.matcher, refs, augs_per_ref=16)  # (1,256)\n",
    "\n",
    "        # video\n",
    "        vpath = video_path_for_episode(data_root, video_id)\n",
    "        cap = cv2.VideoCapture(vpath)\n",
    "        if not cap.isOpened():\n",
    "            raise RuntimeError(f\"Cannot open video: {vpath}\")\n",
    "\n",
    "        tracker = SingleTargetTracker(\n",
    "            tau_high=self.tau_high, tau_low=self.tau_low, assoc_lambda=self.assoc_lambda,\n",
    "            search_roi_pad=self.roi_pad, max_lost=self.max_lost,\n",
    "            min_commit=self.min_commit, gap_fill=self.gap_fill\n",
    "        )\n",
    "\n",
    "        frame_idx = 0\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "            if self.frame_stride > 1 and (frame_idx % self.frame_stride) != 0:\n",
    "                frame_idx += 1\n",
    "                continue\n",
    "\n",
    "            # proposals\n",
    "            props = self.props(frame)  # list of (x1,y1,x2,y2)\n",
    "\n",
    "            # crops & embeddings\n",
    "            crops = [frame[y1:y2, x1:x2] for (x1,y1,x2,y2) in props]\n",
    "            embs  = self._embed_crops(crops)\n",
    "            sims  = self._cosine_scores(embs, template)\n",
    "\n",
    "            # final NMS by similarity\n",
    "            keep = nms_xyxy(props, sims, iou_thr=self.nms_final_iou)\n",
    "            props_kept = [props[i] for i in keep]\n",
    "            sims_kept  = [float(sims[i]) for i in keep]\n",
    "\n",
    "            tracker.update(frame_idx, frame.shape, props_kept, sims_kept)\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        dets = tracker.finalize()  # flat list of dicts with frame,x1,y1,x2,y2\n",
    "        segments = self._segmentize(dets, max_gap=1)\n",
    "        return {\"video_id\": video_id, \"detections\": segments}\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# CLI / main\n",
    "# ------------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_root\", type=str, required=True, help=\"Path to dataset root (containing samples/ and annotations/)\")\n",
    "    parser.add_argument(\"--out\", type=str, default=\"predictions.json\", help=\"Output predictions JSON\")\n",
    "    parser.add_argument(\"--eval\", action=\"store_true\", help=\"Compute ST-IoU vs annotations if available\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=1337)\n",
    "    # quick knobs\n",
    "    parser.add_argument(\"--conf\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--nms_iou\", type=float, default=0.7)\n",
    "    parser.add_argument(\"--imgsz\", type=int, default=640)\n",
    "    parser.add_argument(\"--max_props\", type=int, default=200)\n",
    "    parser.add_argument(\"--tau_high\", type=float, default=0.45)\n",
    "    parser.add_argument(\"--tau_low\", type=float, default=0.35)\n",
    "    parser.add_argument(\"--assoc_lambda\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--roi_pad\", type=float, default=0.35)\n",
    "    parser.add_argument(\"--max_lost\", type=int, default=10)\n",
    "    parser.add_argument(\"--min_commit\", type=int, default=3)\n",
    "    parser.add_argument(\"--gap_fill\", type=int, default=2)\n",
    "    parser.add_argument(\"--frame_stride\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    predictor = Predictor(\n",
    "        conf=args.conf, nms_iou=args.nms_iou, imgsz=args.imgsz, max_props=args.max_props,\n",
    "        tau_high=args.tau_high, tau_low=args.tau_low, assoc_lambda=args.assoc_lambda,\n",
    "        roi_pad=args.roi_pad, max_lost=args.max_lost, min_commit=args.min_commit, gap_fill=args.gap_fill,\n",
    "        frame_stride=args.frame_stride\n",
    "    )\n",
    "\n",
    "    video_ids = find_episodes(args.data_root)\n",
    "    print(f\"[INFO] Found {len(video_ids)} episodes:\", video_ids)\n",
    "\n",
    "    preds = []\n",
    "    for vid in video_ids:\n",
    "        t0 = time.time()\n",
    "        entry = predictor.run_episode(args.data_root, vid)\n",
    "        dt = time.time() - t0\n",
    "        preds.append(entry)\n",
    "        print(f\"[DONE] {vid}: {sum(len(s['bboxes']) for s in entry['detections'])} boxes, {len(entry['detections'])} segments, {dt:.1f}s\")\n",
    "\n",
    "    with open(args.out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(preds, f, indent=2)\n",
    "    print(f\"[SAVE] predictions -> {args.out}\")\n",
    "\n",
    "    if args.eval:\n",
    "        gt_entries = load_annotations_json(args.data_root)\n",
    "        if gt_entries:\n",
    "            score = st_iou_mean(gt_entries, preds, video_ids)\n",
    "            print(f\"[ST-IoU] mean over {len(video_ids)} videos: {score:.4f}\")\n",
    "        else:\n",
    "            print(\"[WARN] annotations/annotations.json not found; skipping ST-IoU.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e79b1d0",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee64904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1511 annotated frames to viz/Lifering_1\n",
      "Wrote video: viz\\Lifering_1\\overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "# viz_overlays.py\n",
    "# Visualize predictions vs ground-truth for the drone instance-localization task.\n",
    "# - Handles MULTIPLE GT segments per video_id (annotations: [ {\"bboxes\":[...]}, {\"bboxes\":[...]} , ... ])\n",
    "# - Accepts predictions in either flat or segmented format:\n",
    "#     {\"video_id\": \"...\", \"detections\": [{\"bboxes\":[...]}, {\"bboxes\":[...]}]}\n",
    "#     or {\"video_id\": \"...\", \"detections\": {\"bboxes\":[...]}}\n",
    "#     or {\"video_id\": \"...\", \"detections\": [...] }  # where the list is bboxes directly\n",
    "#\n",
    "# Output: per-frame PNGs (only for frames with any GT/PRED box) and an overlay MP4 (optional).\n",
    "\n",
    "import json, os, cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG — edit these paths\n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"train/samples/Lifering_1/drone_video.mp4\"\n",
    "VIDEO_ID   = \"Lifering_1\"\n",
    "\n",
    "GT_JSON    = \"train/annotations/annotations.json\"   # set to None to skip GT\n",
    "PRED_JSON  = \"predictions_v3.json\"               # set to None to skip predictions\n",
    "\n",
    "OUT_DIR        = f\"viz/{VIDEO_ID}\"\n",
    "WRITE_VIDEO    = True\n",
    "VIDEO_FILENAME = \"overlay.mp4\"\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def safe_load_json(path):\n",
    "    if path is None or not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _extract_segments(record, key):\n",
    "    \"\"\"\n",
    "    record[key] might be:\n",
    "      - a list of segments [{ \"bboxes\": [...] }, {\"bboxes\": [...]}, ...]\n",
    "      - a single dict with \"bboxes\"\n",
    "      - a flat list of dicts that are actually bboxes\n",
    "    Return a list of segments, each as {\"bboxes\": [ {frame,x1,y1,x2,y2}, ... ]}.\n",
    "    \"\"\"\n",
    "    segs = record.get(key, [])\n",
    "    # normalize\n",
    "    if isinstance(segs, dict) and \"bboxes\" in segs:\n",
    "        return [ {\"bboxes\": segs[\"bboxes\"]} ]\n",
    "    if isinstance(segs, list):\n",
    "        # Could be list of segments OR list of bbox dicts\n",
    "        if len(segs) > 0 and isinstance(segs[0], dict) and \"bboxes\" in segs[0]:\n",
    "            # already list of segments\n",
    "            return segs\n",
    "        elif len(segs) > 0 and isinstance(segs[0], dict) and \"frame\" in segs[0]:\n",
    "            # it's a flat list of bbox dicts\n",
    "            return [ {\"bboxes\": segs} ]\n",
    "        else:\n",
    "            # empty or unknown — treat as no segments\n",
    "            return []\n",
    "    # anything else → no segments\n",
    "    return []\n",
    "\n",
    "\n",
    "def index_bboxes_all_segments(entries, video_id, key=\"annotations\"):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      - frame_to_boxes: { frame_idx: [(x1,y1,x2,y2), ...] }\n",
    "      - segments: [ {\"bboxes\":[ {...}, {...}, ... ]}, ... ]\n",
    "    Works for both GT (\"annotations\") and predictions (\"detections\").\n",
    "    \"\"\"\n",
    "    if not entries:\n",
    "        return {}, []\n",
    "    by_vid = {e.get(\"video_id\"): e for e in entries if \"video_id\" in e}\n",
    "    if video_id not in by_vid:\n",
    "        return {}, []\n",
    "    rec = by_vid[video_id]\n",
    "\n",
    "    segments = _extract_segments(rec, key)\n",
    "    frame_to_boxes = {}\n",
    "\n",
    "    for seg in segments:\n",
    "        bbs = seg.get(\"bboxes\", [])\n",
    "        for b in bbs:\n",
    "            k  = int(b[\"frame\"])\n",
    "            x1 = int(b[\"x1\"]); y1 = int(b[\"y1\"]); x2 = int(b[\"x2\"]); y2 = int(b[\"y2\"])\n",
    "            frame_to_boxes.setdefault(k, []).append((x1, y1, x2, y2))\n",
    "\n",
    "    return frame_to_boxes, segments\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    xx1, yy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    xx2, yy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    w, h = max(0, xx2 - xx1), max(0, yy2 - yy1)\n",
    "    inter = w * h\n",
    "    if inter <= 0:\n",
    "        return 0.0\n",
    "    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n",
    "    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n",
    "    return float(inter) / float(area_a + area_b - inter + 1e-6)\n",
    "\n",
    "\n",
    "def main():\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gt_entries   = safe_load_json(GT_JSON) or []\n",
    "    pred_entries = safe_load_json(PRED_JSON) or []\n",
    "\n",
    "    gt_frame2boxes, _   = index_bboxes_all_segments(gt_entries,   VIDEO_ID, key=\"annotations\")\n",
    "    pred_frame2boxes, _ = index_bboxes_all_segments(pred_entries, VIDEO_ID, key=\"detections\")\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "\n",
    "    writer = None\n",
    "    if WRITE_VIDEO:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(str(Path(OUT_DIR, VIDEO_FILENAME)), fourcc, fps, (w, h))\n",
    "\n",
    "    frame_id = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        gts   = gt_frame2boxes.get(frame_id, [])\n",
    "        preds = pred_frame2boxes.get(frame_id, [])\n",
    "\n",
    "        if gts or preds:\n",
    "            vis = frame.copy()\n",
    "\n",
    "            # Draw GT (RED)\n",
    "            for (x1, y1, x2, y2) in gts:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(vis, \"GT\", (x1, max(0, y1 - 6)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Draw predictions (GREEN) and best IoU vs any GT on this frame\n",
    "            for (x1, y1, x2, y2) in preds:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label = \"PRED\"\n",
    "                if gts:\n",
    "                    best_iou = max(iou((x1, y1, x2, y2), g) for g in gts)\n",
    "                    label += f\" IoU={best_iou:.2f}\"\n",
    "                cv2.putText(vis, label, (x1, min(h - 6, y2 + 18)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Header text\n",
    "            legend = \"Legend: PRED=Green, GT=Red\"\n",
    "            cv2.putText(vis, legend, (10, 26),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(vis, f\"video_id={VIDEO_ID}  frame={frame_id}\",\n",
    "                        (10, 52), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Save this annotated frame as PNG\n",
    "            out_png = Path(OUT_DIR, f\"frame_{frame_id:06d}.png\")\n",
    "            cv2.imwrite(str(out_png), vis)\n",
    "            saved += 1\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.write(vis)\n",
    "        else:\n",
    "            # If you want a full-length overlay video, write the raw frame too\n",
    "            if writer is not None:\n",
    "                writer.write(frame)\n",
    "\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print(f\"Saved {saved} annotated frames to {OUT_DIR}\")\n",
    "    if WRITE_VIDEO:\n",
    "        print(f\"Wrote video: {Path(OUT_DIR, VIDEO_FILENAME)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c9995",
   "metadata": {},
   "source": [
    "## Public test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0edd2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1199 annotated frames to viz/LifeJacket_0\n",
      "Wrote video: viz\\LifeJacket_0\\overlay.mp4\n"
     ]
    }
   ],
   "source": [
    "# viz_overlays.py\n",
    "# Visualize predictions vs ground-truth for the drone instance-localization task.\n",
    "# - Handles MULTIPLE GT segments per video_id (annotations: [ {\"bboxes\":[...]}, {\"bboxes\":[...]} , ... ])\n",
    "# - Accepts predictions in either flat or segmented format:\n",
    "#     {\"video_id\": \"...\", \"detections\": [{\"bboxes\":[...]}, {\"bboxes\":[...]}]}\n",
    "#     or {\"video_id\": \"...\", \"detections\": {\"bboxes\":[...]}}\n",
    "#     or {\"video_id\": \"...\", \"detections\": [...] }  # where the list is bboxes directly\n",
    "#\n",
    "# Output: per-frame PNGs (only for frames with any GT/PRED box) and an overlay MP4 (optional).\n",
    "\n",
    "import json, os, cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG — edit these paths\n",
    "# -----------------------------\n",
    "VIDEO_PATH = \"samples/LifeJacket_0/drone_video.mp4\"\n",
    "VIDEO_ID   = \"LifeJacket_0\"\n",
    "\n",
    "GT_JSON    = None   # set to None to skip GT\n",
    "PRED_JSON  = \"predictions_v5.json\"               # set to None to skip predictions\n",
    "\n",
    "OUT_DIR        = f\"viz/{VIDEO_ID}\"\n",
    "WRITE_VIDEO    = True\n",
    "VIDEO_FILENAME = \"overlay.mp4\"\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def safe_load_json(path):\n",
    "    if path is None or not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _extract_segments(record, key):\n",
    "    \"\"\"\n",
    "    record[key] might be:\n",
    "      - a list of segments [{ \"bboxes\": [...] }, {\"bboxes\": [...]}, ...]\n",
    "      - a single dict with \"bboxes\"\n",
    "      - a flat list of dicts that are actually bboxes\n",
    "    Return a list of segments, each as {\"bboxes\": [ {frame,x1,y1,x2,y2}, ... ]}.\n",
    "    \"\"\"\n",
    "    segs = record.get(key, [])\n",
    "    # normalize\n",
    "    if isinstance(segs, dict) and \"bboxes\" in segs:\n",
    "        return [ {\"bboxes\": segs[\"bboxes\"]} ]\n",
    "    if isinstance(segs, list):\n",
    "        # Could be list of segments OR list of bbox dicts\n",
    "        if len(segs) > 0 and isinstance(segs[0], dict) and \"bboxes\" in segs[0]:\n",
    "            # already list of segments\n",
    "            return segs\n",
    "        elif len(segs) > 0 and isinstance(segs[0], dict) and \"frame\" in segs[0]:\n",
    "            # it's a flat list of bbox dicts\n",
    "            return [ {\"bboxes\": segs} ]\n",
    "        else:\n",
    "            # empty or unknown — treat as no segments\n",
    "            return []\n",
    "    # anything else → no segments\n",
    "    return []\n",
    "\n",
    "\n",
    "def index_bboxes_all_segments(entries, video_id, key=\"annotations\"):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "      - frame_to_boxes: { frame_idx: [(x1,y1,x2,y2), ...] }\n",
    "      - segments: [ {\"bboxes\":[ {...}, {...}, ... ]}, ... ]\n",
    "    Works for both GT (\"annotations\") and predictions (\"detections\").\n",
    "    \"\"\"\n",
    "    if not entries:\n",
    "        return {}, []\n",
    "    by_vid = {e.get(\"video_id\"): e for e in entries if \"video_id\" in e}\n",
    "    if video_id not in by_vid:\n",
    "        return {}, []\n",
    "    rec = by_vid[video_id]\n",
    "\n",
    "    segments = _extract_segments(rec, key)\n",
    "    frame_to_boxes = {}\n",
    "\n",
    "    for seg in segments:\n",
    "        bbs = seg.get(\"bboxes\", [])\n",
    "        for b in bbs:\n",
    "            k  = int(b[\"frame\"])\n",
    "            x1 = int(b[\"x1\"]); y1 = int(b[\"y1\"]); x2 = int(b[\"x2\"]); y2 = int(b[\"y2\"])\n",
    "            frame_to_boxes.setdefault(k, []).append((x1, y1, x2, y2))\n",
    "\n",
    "    return frame_to_boxes, segments\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    xx1, yy1 = max(ax1, bx1), max(ay1, by1)\n",
    "    xx2, yy2 = min(ax2, bx2), min(ay2, by2)\n",
    "    w, h = max(0, xx2 - xx1), max(0, yy2 - yy1)\n",
    "    inter = w * h\n",
    "    if inter <= 0:\n",
    "        return 0.0\n",
    "    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n",
    "    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n",
    "    return float(inter) / float(area_a + area_b - inter + 1e-6)\n",
    "\n",
    "\n",
    "def main():\n",
    "    Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gt_entries   = safe_load_json(GT_JSON) or []\n",
    "    pred_entries = safe_load_json(PRED_JSON) or []\n",
    "\n",
    "    gt_frame2boxes, _   = index_bboxes_all_segments(gt_entries,   VIDEO_ID, key=\"annotations\")\n",
    "    pred_frame2boxes, _ = index_bboxes_all_segments(pred_entries, VIDEO_ID, key=\"detections\")\n",
    "\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open video: {VIDEO_PATH}\")\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "\n",
    "    writer = None\n",
    "    if WRITE_VIDEO:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(str(Path(OUT_DIR, VIDEO_FILENAME)), fourcc, fps, (w, h))\n",
    "\n",
    "    frame_id = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        gts   = gt_frame2boxes.get(frame_id, [])\n",
    "        preds = pred_frame2boxes.get(frame_id, [])\n",
    "\n",
    "        if gts or preds:\n",
    "            vis = frame.copy()\n",
    "\n",
    "            # Draw GT (RED)\n",
    "            for (x1, y1, x2, y2) in gts:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(vis, \"GT\", (x1, max(0, y1 - 6)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Draw predictions (GREEN) and best IoU vs any GT on this frame\n",
    "            for (x1, y1, x2, y2) in preds:\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                label = \"PRED\"\n",
    "                if gts:\n",
    "                    best_iou = max(iou((x1, y1, x2, y2), g) for g in gts)\n",
    "                    label += f\" IoU={best_iou:.2f}\"\n",
    "                cv2.putText(vis, label, (x1, min(h - 6, y2 + 18)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Header text\n",
    "            legend = \"Legend: PRED=Green, GT=Red\"\n",
    "            cv2.putText(vis, legend, (10, 26),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.putText(vis, f\"video_id={VIDEO_ID}  frame={frame_id}\",\n",
    "                        (10, 52), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Save this annotated frame as PNG\n",
    "            out_png = Path(OUT_DIR, f\"frame_{frame_id:06d}.png\")\n",
    "            cv2.imwrite(str(out_png), vis)\n",
    "            saved += 1\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.write(vis)\n",
    "        else:\n",
    "            # If you want a full-length overlay video, write the raw frame too\n",
    "            if writer is not None:\n",
    "                writer.write(frame)\n",
    "\n",
    "        frame_id += 1\n",
    "\n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print(f\"Saved {saved} annotated frames to {OUT_DIR}\")\n",
    "    if WRITE_VIDEO:\n",
    "        print(f\"Wrote video: {Path(OUT_DIR, VIDEO_FILENAME)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329055ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
