{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22837e6",
   "metadata": {},
   "source": [
    "# Week 7 — Introduction to TensorBoard (Keras/TensorFlow)\n",
    "This notebook will guide you through the 4 tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed921f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, os\n",
    "print('TF version:', tf.__version__)\n",
    "os.makedirs('runs', exist_ok=True)\n",
    "os.makedirs('artifacts', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a60ce",
   "metadata": {},
   "source": [
    "## Task 1 — Basic TensorBoard Setup and Logging\n",
    "We use MNIST to focus on TensorBoard features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf70d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Train: (54000, 28, 28) (54000,) | Val: (6000, 28, 28) (6000,) | Test: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np, os\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\")/255.0\n",
    "x_test  = x_test.astype(\"float32\")/255.0\n",
    "VAL_RATIO = 0.1\n",
    "n_val = int(len(x_train)*VAL_RATIO)\n",
    "x_val, y_val = x_train[:n_val], y_train[:n_val]\n",
    "x_train, y_train = x_train[n_val:], y_train[n_val:]\n",
    "print(\"Train:\", x_train.shape, y_train.shape, \"| Val:\", x_val.shape, y_val.shape, \"| Test:\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b14e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_mlp(input_shape=(28,28), num_classes=10, hidden_units=128, dropout=0.2):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(name=\"flatten\"),\n",
    "        layers.Dense(hidden_units, activation=\"relu\", name=\"dense_1\"),\n",
    "        layers.Dropout(dropout, name=\"dropout_1\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\", name=\"logits\")\n",
    "    ], name=f\"mlp_{hidden_units}\")\n",
    "    return model\n",
    "\n",
    "def compile_model(model, learning_rate=1e-3):\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "def get_run_dir(prefix, **hparams):\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tag = \"__\".join([f\"{k}-{v}\" for k,v in hparams.items()])\n",
    "    dirname = f\"{prefix}-{stamp}\" + (f\"__{tag}\" if tag else \"\")\n",
    "    run_dir = os.path.join(\"runs\", dirname)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    return run_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9fec3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mlp_128\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"mlp_128\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to: C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\n",
      "Epoch 1/3\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.8796 - loss: 0.4215 - val_accuracy: 0.9470 - val_loss: 0.1970\n",
      "Epoch 2/3\n",
      "422/422 - 3s - 6ms/step - accuracy: 0.9418 - loss: 0.1993 - val_accuracy: 0.9615 - val_loss: 0.1375\n",
      "Epoch 3/3\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9566 - loss: 0.1492 - val_accuracy: 0.9697 - val_loss: 0.1113\n",
      "Logs -> C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\n",
      "Files: ['train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = compile_model(make_mlp(hidden_units=128), learning_rate=1e-3)\n",
    "mlp.summary()\n",
    "run_dir = get_run_dir(\"task1_mlp\", lr=1e-3, hu=128)\n",
    "\n",
    "import os, uuid\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "# Choose a short, ASCII-only base dir\n",
    "BASE = Path(\"C:/tb_logs\")   # or: Path(os.getenv(\"TEMP\", \"C:/Temp\")) / \"tb_logs\"\n",
    "BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_dir = BASE / f\"task1_mlp-{uuid.uuid4().hex}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Logging to:\", run_dir)\n",
    "\n",
    "tb = keras.callbacks.TensorBoard(\n",
    "    log_dir=str(run_dir), histogram_freq=1, write_graph=True\n",
    ")\n",
    "#tb = keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=1, write_graph=True)\n",
    "history = mlp.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                  epochs=3, batch_size=128, callbacks=[tb], verbose=2)\n",
    "print(\"Logs ->\", run_dir)\n",
    "print(\"Files:\", os.listdir(run_dir)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfa01e",
   "metadata": {},
   "source": [
    "**Open TensorBoard:** In a terminal, run: `tensorboard --logdir runs --port 6006` and open http://localhost:6006 → Scalars tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec65ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_dir = C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\n",
      "C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\n",
      "C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\\train\n",
      "  - events.out.tfevents.1761722940.MSI.35616.0.v2\n",
      "C:\\tb_logs\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\\validation\n",
      "  - events.out.tfevents.1761722945.MSI.35616.1.v2\n",
      "\n",
      "# of event files: 2\n",
      "['C:\\\\tb_logs\\\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\\\\train\\\\events.out.tfevents.1761722940.MSI.35616.0.v2',\n",
      " 'C:\\\\tb_logs\\\\task1_mlp-0488b2988ca64df29cdd5970fe8c5b20\\\\validation\\\\events.out.tfevents.1761722945.MSI.35616.1.v2']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pprint\n",
    "print(\"run_dir =\", run_dir)\n",
    "\n",
    "# What’s inside that folder?\n",
    "for root, dirs, files in os.walk(str(run_dir)):\n",
    "    print(root)\n",
    "    for f in files:\n",
    "        print(\"  -\", f)\n",
    "\n",
    "# Specifically look for TF event files\n",
    "event_files = glob.glob(os.path.join(str(run_dir), \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
    "print(\"\\n# of event files:\", len(event_files))\n",
    "pprint.pp(event_files[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed2196",
   "metadata": {},
   "source": [
    "# Task 2 — Training Metrics Visualization\n",
    "We compare learning rates and log custom macro precision/recall/F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f329c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "import numpy as np, os\n",
    "\n",
    "class SklearnMetrics(keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, log_dir):\n",
    "        super().__init__()\n",
    "        self.x_val, self.y_val = val_data\n",
    "        self.writer = tf.summary.create_file_writer(os.path.join(log_dir, \"custom_metrics\"))\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_prob = self.model.predict(self.x_val, verbose=0)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        precision = precision_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        recall    = recall_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1        = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(\"precision_macro\", precision, step=epoch)\n",
    "            tf.summary.scalar(\"recall_macro\", recall, step=epoch)\n",
    "            tf.summary.scalar(\"f1_macro\", f1, step=epoch)\n",
    "        print(f\"[Val epoch {epoch+1}] precision={precision:.4f} recall={recall:.4f} f1={f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10dde6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG_BASE = C:\\tb_logs\n"
     ]
    }
   ],
   "source": [
    "# --- Force a clean ASCII base for ALL logs ---\n",
    "from pathlib import Path\n",
    "import os, json, itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "LOG_BASE = Path(\"C:/tb_logs\").resolve()\n",
    "LOG_BASE.mkdir(parents=True, exist_ok=True)\n",
    "print(\"LOG_BASE =\", LOG_BASE)\n",
    "\n",
    "def get_run_dir(prefix, **hparams):\n",
    "    from datetime import datetime\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tag = \"__\".join([f\"{k}-{v}\" for k,v in hparams.items()])\n",
    "    name = f\"{prefix}-{stamp}\" + (f\"__{tag}\" if tag else \"\")\n",
    "    run_dir = LOG_BASE / name\n",
    "    # ensure directory exists and is a dir\n",
    "    if run_dir.exists() and not run_dir.is_dir():\n",
    "        run_dir.unlink()\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return run_dir  # return a Path\n",
    "\n",
    "# Re-define the callback to create its subdir safely, using POSIX paths\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "class SklearnMetrics(keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, log_dir):\n",
    "        super().__init__()\n",
    "        self.x_val, self.y_val = val_data\n",
    "        cm_dir = Path(log_dir) / \"custom_metrics\"\n",
    "        if cm_dir.exists() and not cm_dir.is_dir():\n",
    "            cm_dir.unlink()\n",
    "        # make sure it exists and pass a POSIX string to TF\n",
    "        tf.io.gfile.makedirs(cm_dir.as_posix())\n",
    "        self.writer = tf.summary.create_file_writer(cm_dir.as_posix())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_prob = self.model.predict(self.x_val, verbose=0)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        precision = precision_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        recall    = recall_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        f1        = f1_score(self.y_val, y_pred, average=\"macro\", zero_division=0)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(\"precision_macro\", precision, step=epoch)\n",
    "            tf.summary.scalar(\"recall_macro\",    recall,    step=epoch)\n",
    "            tf.summary.scalar(\"f1_macro\",        f1,        step=epoch)\n",
    "\n",
    "def train_one_run(lr=1e-3, hidden_units=128, batch_size=128, epochs=5, prefix=\"task2_mlp\"):\n",
    "    model = compile_model(make_mlp(hidden_units=hidden_units), learning_rate=lr)\n",
    "\n",
    "    run_dir = get_run_dir(prefix, lr=lr, hu=hidden_units, bs=batch_size)\n",
    "    # Ensure the Keras TB callback also gets a POSIX (ASCII) path:\n",
    "    tb = keras.callbacks.TensorBoard(log_dir=run_dir.as_posix(), histogram_freq=1, write_graph=True)\n",
    "\n",
    "    callbacks = [\n",
    "        tb,\n",
    "        SklearnMetrics(val_data=(x_val, y_val), log_dir=run_dir),\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            (Path(\"artifacts\") / f\"{prefix}_lr{lr}_hu{hidden_units}_bs{batch_size}.keras\").as_posix(),\n",
    "            monitor=\"val_accuracy\", save_best_only=True\n",
    "        ),\n",
    "        keras.callbacks.CSVLogger((run_dir / \"history.csv\").as_posix())\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs, batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    print(\"Logs ->\", run_dir)\n",
    "    return run_dir, history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858fc40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9173 - loss: 0.2711 - val_accuracy: 0.9607 - val_loss: 0.1305\n",
      "Epoch 2/6\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9485 - loss: 0.1687 - val_accuracy: 0.9678 - val_loss: 0.1123\n",
      "Epoch 3/6\n",
      "422/422 - 4s - 8ms/step - accuracy: 0.9571 - loss: 0.1417 - val_accuracy: 0.9682 - val_loss: 0.1057\n",
      "Epoch 4/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9614 - loss: 0.1229 - val_accuracy: 0.9678 - val_loss: 0.1201\n",
      "Epoch 5/6\n",
      "422/422 - 4s - 8ms/step - accuracy: 0.9647 - loss: 0.1162 - val_accuracy: 0.9657 - val_loss: 0.1394\n",
      "Epoch 6/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9657 - loss: 0.1122 - val_accuracy: 0.9702 - val_loss: 0.1246\n",
      "Logs -> C:\\tb_logs\\task2_mlp-20251029-145733__lr-0.01__hu-128__bs-128\n",
      "Epoch 1/6\n",
      "422/422 - 5s - 11ms/step - accuracy: 0.8806 - loss: 0.4249 - val_accuracy: 0.9462 - val_loss: 0.1977\n",
      "Epoch 2/6\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9401 - loss: 0.2055 - val_accuracy: 0.9592 - val_loss: 0.1425\n",
      "Epoch 3/6\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9547 - loss: 0.1569 - val_accuracy: 0.9652 - val_loss: 0.1152\n",
      "Epoch 4/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9628 - loss: 0.1268 - val_accuracy: 0.9697 - val_loss: 0.0985\n",
      "Epoch 5/6\n",
      "422/422 - 3s - 7ms/step - accuracy: 0.9673 - loss: 0.1095 - val_accuracy: 0.9718 - val_loss: 0.0891\n",
      "Epoch 6/6\n",
      "422/422 - 5s - 13ms/step - accuracy: 0.9715 - loss: 0.0938 - val_accuracy: 0.9743 - val_loss: 0.0836\n",
      "Logs -> C:\\tb_logs\\task2_mlp-20251029-145756__lr-0.001__hu-128__bs-128\n",
      "Compare runs in TensorBoard: C:\\tb_logs\\task2_mlp-20251029-145733__lr-0.01__hu-128__bs-128 vs C:\\tb_logs\\task2_mlp-20251029-145756__lr-0.001__hu-128__bs-128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_a, hist_a = train_one_run(lr=1e-2, hidden_units=128, batch_size=128, epochs=6, prefix=\"task2_mlp\")\n",
    "run_b, hist_b = train_one_run(lr=1e-3, hidden_units=128, batch_size=128, epochs=6, prefix=\"task2_mlp\")\n",
    "print(\"Compare runs in TensorBoard:\", run_a, \"vs\", run_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17020371",
   "metadata": {},
   "source": [
    "**Write your insights here:** Which LR converged faster? Do custom metrics differ from accuracy? Any overfitting/underfitting signs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f534425",
   "metadata": {},
   "source": [
    "# Task 2 — Training Progress & Metric Insights\n",
    "\n",
    "## Scalars (training)\n",
    "- **Accuracy (`epoch_accuracy`)** rises steadily from ~**0.91–0.92** at epoch 0 to ~**0.96** by epoch 5.\n",
    "  - Shape shows **fast early gains** (epochs 0→2) and **diminishing returns** after epoch 3.\n",
    "- **Loss (`epoch_loss`)** drops from ~**0.24–0.29** at epoch 0 to ~**0.12** by epoch 5.\n",
    "  - The curve is smooth and monotonic ↓, indicating **stable optimization** (no spikes/instability).\n",
    "\n",
    "**Takeaway:** The learning rate is well-tuned for this model/data; training progresses smoothly with no obvious divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Custom Validation Metrics (macro)\n",
    "*(logged via the `SklearnMetrics` callback)*\n",
    "\n",
    "- **F1 (macro):** climbs from ~**0.94–0.96** at epoch 0 to ~**0.969–0.971** at epoch 5.\n",
    "- **Precision (macro):** increases from ~**0.95–0.96** to ~**0.969–0.970** by epoch 5.\n",
    "- **Recall (macro):** improves from ~**0.94–0.95** to ~**0.970–0.972** by epoch 5.\n",
    "\n",
    "**Class balance insight:** Precision, recall, and F1 remain **close to each other** throughout training, suggesting **no major class imbalance issues** and that the model is **not favoring specific digits**. Slightly faster growth in precision/recall early on indicates the decision boundaries improve quickly, then refine more slowly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be652752",
   "metadata": {},
   "source": [
    "# Task 3 — Model Architecture Visualization (Graphs tab)\n",
    "We log graphs for a deeper MLP and a simple CNN and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e94877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trace already enabled\n",
      "Epoch 1/3\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9031 - loss: 0.3373 - val_accuracy: 0.9623 - val_loss: 0.1332\n",
      "Epoch 2/3\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9611 - loss: 0.1327 - val_accuracy: 0.9725 - val_loss: 0.0938\n",
      "Epoch 3/3\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9720 - loss: 0.0932 - val_accuracy: 0.9780 - val_loss: 0.0739\n",
      "Graphs/logs -> C:/tb_logs/task3_deeper_mlp-20251029-150617\n",
      "Epoch 1/3\n",
      "422/422 - 12s - 28ms/step - accuracy: 0.9260 - loss: 0.2429 - val_accuracy: 0.9828 - val_loss: 0.0577\n",
      "Epoch 2/3\n",
      "422/422 - 10s - 23ms/step - accuracy: 0.9794 - loss: 0.0666 - val_accuracy: 0.9850 - val_loss: 0.0482\n",
      "Epoch 3/3\n",
      "422/422 - 20s - 47ms/step - accuracy: 0.9857 - loss: 0.0469 - val_accuracy: 0.9887 - val_loss: 0.0414\n",
      "Graphs/logs -> C:/tb_logs/task3_cnn-20251029-150629\n",
      "Graphs written under: C:\\tb_logs\\task3_deeper_mlp-20251029-150617 and C:\\tb_logs\\task3_cnn-20251029-150629\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Saved diagrams to artifacts/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf, os\n",
    "\n",
    "def make_deeper_mlp(input_shape=(28,28), num_classes=10):\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ], name=\"deeper_mlp\")\n",
    "\n",
    "def make_cnn(input_shape=(28,28,1), num_classes=10):\n",
    "    return keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Reshape((28,28,1)),\n",
    "        layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ], name=\"simple_cnn\")\n",
    "\n",
    "# --- Patch for Task 3: make sure TF gets a string logdir ---\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ensure a clean ASCII base on Windows\n",
    "LOG_BASE = Path(\"C:/tb_logs\").resolve()\n",
    "LOG_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_run_dir(prefix):\n",
    "    from datetime import datetime\n",
    "    name = f\"{prefix}-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    rd = (LOG_BASE / name)\n",
    "    rd.mkdir(parents=True, exist_ok=True)\n",
    "    return rd  # Path\n",
    "\n",
    "def train_with_graph(model, run_prefix, epochs=3):\n",
    "    model = compile_model(model, learning_rate=1e-3)\n",
    "    run_dir = get_run_dir(run_prefix)               # Path\n",
    "    logdir  = run_dir.as_posix()                    # <-- convert to string\n",
    "    tf.io.gfile.makedirs(logdir)\n",
    "\n",
    "    tb = keras.callbacks.TensorBoard(\n",
    "        log_dir=logdir, histogram_freq=1, write_graph=True\n",
    "    )\n",
    "\n",
    "    # Force graph trace to appear in the Graphs tab\n",
    "    tf.summary.trace_on(graph=True, profiler=False)\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs, batch_size=128,\n",
    "        callbacks=[tb], verbose=2\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: pass string path to the writer\n",
    "    with tf.summary.create_file_writer(logdir).as_default():\n",
    "        tf.summary.trace_export(name=\"graph_trace\", step=0)\n",
    "\n",
    "    print(\"Graphs/logs ->\", logdir)\n",
    "    return run_dir\n",
    "\n",
    "\n",
    "run_deeper = train_with_graph(make_deeper_mlp(), \"task3_deeper_mlp\", epochs=3)\n",
    "run_cnn    = train_with_graph(make_cnn(), \"task3_cnn\", epochs=3)\n",
    "print(\"Graphs written under:\", run_deeper, \"and\", run_cnn)\n",
    "\n",
    "# Optional: export .png diagrams if graphviz+pydot are available\n",
    "try:\n",
    "    keras.utils.plot_model(make_deeper_mlp(), to_file=os.path.join(\"artifacts\", \"deeper_mlp.png\"), show_shapes=True)\n",
    "    keras.utils.plot_model(make_cnn(), to_file=os.path.join(\"artifacts\", \"simple_cnn.png\"), show_shapes=True)\n",
    "    print(\"Saved diagrams to artifacts/\")\n",
    "except Exception as e:\n",
    "    print(\"plot_model skipped:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895807a5",
   "metadata": {},
   "source": [
    "**Explain the structures:** Deeper MLP (dense-only) vs CNN (convs + pooling). In Graphs tab, inspect the tensor flow differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3359d8",
   "metadata": {},
   "source": [
    "# Task 3 — Model Architecture & Data Flow\n",
    "\n",
    "## 1) Deeper MLP (Dense-only)\n",
    "\n",
    "**Purpose:** Fast baseline for 2D images when you’re OK with flattening (loses spatial structure).\n",
    "\n",
    "**Layers & data flow**\n",
    "1. **Input**: `(28, 28)` grayscale image  \n",
    "2. **Flatten** → `(784,)` — collapses H×W into a 1D vector (destroys locality)\n",
    "3. **Dense(256, ReLU)** — learns global combinations of pixels\n",
    "4. **Dense(128, ReLU)** — refines higher-level combinations\n",
    "5. **Dropout(0.3)** — regularizes (randomly zeroes activations during training)\n",
    "6. **Dense(10, Softmax)** — class probabilities for digits 0–9\n",
    "\n",
    "**Key characteristics**\n",
    "- **Pros:** Simple, fast to train, good for demos/ablation and tabular-like inputs.  \n",
    "- **Cons:** Ignores spatial structure; must rely on many parameters to “rediscover” locality.  \n",
    "- **When to use:** Quick baselines, sanity checks, or when input is already 1D features.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Simple CNN (Convolutional Neural Network)\n",
    "\n",
    "**Purpose:** Exploit image **spatial locality** and **translation invariance** for better vision performance.\n",
    "\n",
    "**Layers & data flow**\n",
    "1. **Input**: `(28, 28)` → **Reshape** to `(28, 28, 1)` (add channel dim)\n",
    "2. **Conv2D(32, 3×3, ReLU, same)** → `(28, 28, 32)`  \n",
    "   - 32 learned 3×3 filters slide over the image to detect local patterns (edges, corners)\n",
    "3. **MaxPool(2×2)** → `(14, 14, 32)`  \n",
    "   - Downsamples; retains strongest activations → translation tolerance\n",
    "4. **Conv2D(64, 3×3, ReLU, same)** → `(14, 14, 64)`  \n",
    "   - Deeper filters capture more complex patterns (strokes, parts of digits)\n",
    "5. **MaxPool(2×2)** → `(7, 7, 64)`\n",
    "6. **Flatten** → `(3136,)`\n",
    "7. **Dense(128, ReLU)** — combines learned local features into global representation\n",
    "8. **Dropout(0.3)** — regularization\n",
    "9. **Dense(10, Softmax)** — class probabilities\n",
    "\n",
    "**Key characteristics**\n",
    "- **Pros:** Uses convolutions to capture **local structure** efficiently; usually higher accuracy on vision tasks with fewer parameters than a similarly strong MLP.  \n",
    "- **Cons:** Slightly more complex; may need more tuning (filters, pooling strategy).  \n",
    "- **When to use:** Any image data; preferred default for vision.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Side-by-Side Comparison\n",
    "\n",
    "| Aspect | Deeper MLP | Simple CNN |\n",
    "|---|---|---|\n",
    "| **Input handling** | Flattens `(28,28)` → `(784,)` | Keeps spatial grid `(28,28,1)` |\n",
    "| **Inductive bias** | None for images; treats all pixels as independent positions | Strong locality & translation bias via convolution/pooling |\n",
    "| **Representation** | Global dense features | Hierarchical local → global features |\n",
    "| **Parameter efficiency** | Typically **more** params for similar accuracy | Often **fewer** params with **better** accuracy |\n",
    "| **Performance on MNIST** | Solid baseline (~96–98% with tuning) | Typically better (often 98–99%+) |\n",
    "| **Overfitting risk** | Higher if widened deeply without regularization | Lower for same capacity due to weight sharing |\n",
    "| **Train speed** | Very fast per step | Slightly slower per step; converges in fewer epochs |\n",
    "| **Best use case** | Quick baselines, 1D/tabular features | Image data (default choice) |\n",
    "\n",
    "**Why CNN wins on images**\n",
    "- Convolutions **share weights** across locations → learn edge/curve detectors once and reuse them everywhere.  \n",
    "- Pooling provides **translation tolerance** and reduces spatial resolution, focusing on salient features.  \n",
    "- MLP must learn similar patterns **independently** across positions after flattening, which is less efficient and less robust.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab4b7b7",
   "metadata": {},
   "source": [
    "# Task 4 — Hyperparameter Tuning and Comparison\n",
    "Grid search over LR, batch size, and hidden units; summarize best run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bba39e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG_BASE = C:\\tb_logs\n"
     ]
    }
   ],
   "source": [
    "# Unify get_run_dir so Task 2 & 4 work, and keep logs in C:/tb_logs\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "LOG_BASE = Path(\"C:/tb_logs\").resolve()\n",
    "LOG_BASE.mkdir(parents=True, exist_ok=True)\n",
    "print(\"LOG_BASE =\", LOG_BASE)\n",
    "\n",
    "def get_run_dir(prefix, **hparams):\n",
    "    stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tag = \"__\".join([f\"{k}-{v}\" for k, v in hparams.items()]) if hparams else \"\"\n",
    "    name = f\"{prefix}-{stamp}\" + (f\"__{tag}\" if tag else \"\")\n",
    "    rd = LOG_BASE / name\n",
    "    # ensure it's a directory\n",
    "    if rd.exists() and not rd.is_dir():\n",
    "        rd.unlink()\n",
    "    rd.mkdir(parents=True, exist_ok=True)\n",
    "    return rd  # return Path\n",
    "\n",
    "# (Optional) small helpers to always pass strings to TF\n",
    "def as_logdir(p: Path) -> str:\n",
    "    return p.as_posix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dd3f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "844/844 - 7s - 9ms/step - accuracy: 0.8982 - loss: 0.3529 - val_accuracy: 0.9552 - val_loss: 0.1637\n",
      "Epoch 2/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9505 - loss: 0.1702 - val_accuracy: 0.9675 - val_loss: 0.1187\n",
      "Epoch 3/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9624 - loss: 0.1267 - val_accuracy: 0.9723 - val_loss: 0.0955\n",
      "Epoch 4/6\n",
      "844/844 - 5s - 6ms/step - accuracy: 0.9696 - loss: 0.1018 - val_accuracy: 0.9762 - val_loss: 0.0835\n",
      "Epoch 5/6\n",
      "844/844 - 5s - 6ms/step - accuracy: 0.9726 - loss: 0.0866 - val_accuracy: 0.9762 - val_loss: 0.0779\n",
      "Epoch 6/6\n",
      "844/844 - 5s - 6ms/step - accuracy: 0.9771 - loss: 0.0746 - val_accuracy: 0.9777 - val_loss: 0.0741\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152303__lr-0.001__hu-128__bs-64\n",
      "Epoch 1/6\n",
      "844/844 - 7s - 9ms/step - accuracy: 0.9117 - loss: 0.3045 - val_accuracy: 0.9628 - val_loss: 0.1388\n",
      "Epoch 2/6\n",
      "844/844 - 7s - 8ms/step - accuracy: 0.9600 - loss: 0.1365 - val_accuracy: 0.9718 - val_loss: 0.0964\n",
      "Epoch 3/6\n",
      "844/844 - 9s - 11ms/step - accuracy: 0.9713 - loss: 0.0973 - val_accuracy: 0.9770 - val_loss: 0.0829\n",
      "Epoch 4/6\n",
      "844/844 - 9s - 11ms/step - accuracy: 0.9764 - loss: 0.0758 - val_accuracy: 0.9763 - val_loss: 0.0781\n",
      "Epoch 5/6\n",
      "844/844 - 11s - 13ms/step - accuracy: 0.9808 - loss: 0.0618 - val_accuracy: 0.9803 - val_loss: 0.0673\n",
      "Epoch 6/6\n",
      "844/844 - 8s - 9ms/step - accuracy: 0.9841 - loss: 0.0515 - val_accuracy: 0.9807 - val_loss: 0.0652\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152337__lr-0.001__hu-256__bs-64\n",
      "Epoch 1/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.8786 - loss: 0.4266 - val_accuracy: 0.9460 - val_loss: 0.2036\n",
      "Epoch 2/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9415 - loss: 0.2054 - val_accuracy: 0.9610 - val_loss: 0.1456\n",
      "Epoch 3/6\n",
      "422/422 - 4s - 8ms/step - accuracy: 0.9562 - loss: 0.1525 - val_accuracy: 0.9667 - val_loss: 0.1155\n",
      "Epoch 4/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9637 - loss: 0.1235 - val_accuracy: 0.9712 - val_loss: 0.0996\n",
      "Epoch 5/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9688 - loss: 0.1066 - val_accuracy: 0.9755 - val_loss: 0.0894\n",
      "Epoch 6/6\n",
      "422/422 - 6s - 13ms/step - accuracy: 0.9731 - loss: 0.0916 - val_accuracy: 0.9748 - val_loss: 0.0842\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152428__lr-0.001__hu-128__bs-128\n",
      "Epoch 1/6\n",
      "422/422 - 8s - 20ms/step - accuracy: 0.9011 - loss: 0.3511 - val_accuracy: 0.9518 - val_loss: 0.1738\n",
      "Epoch 2/6\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9531 - loss: 0.1608 - val_accuracy: 0.9643 - val_loss: 0.1195\n",
      "Epoch 3/6\n",
      "422/422 - 10s - 24ms/step - accuracy: 0.9658 - loss: 0.1148 - val_accuracy: 0.9715 - val_loss: 0.0971\n",
      "Epoch 4/6\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9738 - loss: 0.0890 - val_accuracy: 0.9757 - val_loss: 0.0832\n",
      "Epoch 5/6\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9783 - loss: 0.0742 - val_accuracy: 0.9772 - val_loss: 0.0764\n",
      "Epoch 6/6\n",
      "422/422 - 6s - 14ms/step - accuracy: 0.9814 - loss: 0.0611 - val_accuracy: 0.9778 - val_loss: 0.0718\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152453__lr-0.001__hu-256__bs-128\n",
      "Epoch 1/6\n",
      "844/844 - 9s - 11ms/step - accuracy: 0.8721 - loss: 0.4566 - val_accuracy: 0.9382 - val_loss: 0.2286\n",
      "Epoch 2/6\n",
      "844/844 - 7s - 8ms/step - accuracy: 0.9355 - loss: 0.2279 - val_accuracy: 0.9565 - val_loss: 0.1624\n",
      "Epoch 3/6\n",
      "844/844 - 10s - 12ms/step - accuracy: 0.9496 - loss: 0.1735 - val_accuracy: 0.9642 - val_loss: 0.1327\n",
      "Epoch 4/6\n",
      "844/844 - 7s - 8ms/step - accuracy: 0.9590 - loss: 0.1423 - val_accuracy: 0.9692 - val_loss: 0.1136\n",
      "Epoch 5/6\n",
      "844/844 - 7s - 8ms/step - accuracy: 0.9646 - loss: 0.1217 - val_accuracy: 0.9690 - val_loss: 0.1030\n",
      "Epoch 6/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9687 - loss: 0.1061 - val_accuracy: 0.9733 - val_loss: 0.0922\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152534__lr-0.0005__hu-128__bs-64\n",
      "Epoch 1/6\n",
      "844/844 - 8s - 9ms/step - accuracy: 0.8949 - loss: 0.3762 - val_accuracy: 0.9477 - val_loss: 0.1902\n",
      "Epoch 2/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9485 - loss: 0.1786 - val_accuracy: 0.9637 - val_loss: 0.1330\n",
      "Epoch 3/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9619 - loss: 0.1301 - val_accuracy: 0.9698 - val_loss: 0.1087\n",
      "Epoch 4/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9698 - loss: 0.1032 - val_accuracy: 0.9717 - val_loss: 0.0921\n",
      "Epoch 5/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9751 - loss: 0.0848 - val_accuracy: 0.9758 - val_loss: 0.0818\n",
      "Epoch 6/6\n",
      "844/844 - 6s - 7ms/step - accuracy: 0.9794 - loss: 0.0706 - val_accuracy: 0.9750 - val_loss: 0.0749\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152621__lr-0.0005__hu-256__bs-64\n",
      "Epoch 1/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.8461 - loss: 0.5509 - val_accuracy: 0.9297 - val_loss: 0.2642\n",
      "Epoch 2/6\n",
      "422/422 - 3s - 8ms/step - accuracy: 0.9231 - loss: 0.2688 - val_accuracy: 0.9478 - val_loss: 0.1971\n",
      "Epoch 3/6\n",
      "422/422 - 3s - 7ms/step - accuracy: 0.9401 - loss: 0.2093 - val_accuracy: 0.9572 - val_loss: 0.1596\n",
      "Epoch 4/6\n",
      "422/422 - 3s - 7ms/step - accuracy: 0.9494 - loss: 0.1741 - val_accuracy: 0.9595 - val_loss: 0.1394\n",
      "Epoch 5/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9559 - loss: 0.1507 - val_accuracy: 0.9658 - val_loss: 0.1214\n",
      "Epoch 6/6\n",
      "422/422 - 5s - 12ms/step - accuracy: 0.9613 - loss: 0.1320 - val_accuracy: 0.9690 - val_loss: 0.1103\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152659__lr-0.0005__hu-128__bs-128\n",
      "Epoch 1/6\n",
      "422/422 - 7s - 17ms/step - accuracy: 0.8747 - loss: 0.4535 - val_accuracy: 0.9407 - val_loss: 0.2234\n",
      "Epoch 2/6\n",
      "422/422 - 4s - 10ms/step - accuracy: 0.9396 - loss: 0.2132 - val_accuracy: 0.9565 - val_loss: 0.1600\n",
      "Epoch 3/6\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9539 - loss: 0.1602 - val_accuracy: 0.9665 - val_loss: 0.1281\n",
      "Epoch 4/6\n",
      "422/422 - 4s - 8ms/step - accuracy: 0.9623 - loss: 0.1297 - val_accuracy: 0.9692 - val_loss: 0.1096\n",
      "Epoch 5/6\n",
      "422/422 - 4s - 8ms/step - accuracy: 0.9681 - loss: 0.1090 - val_accuracy: 0.9735 - val_loss: 0.0954\n",
      "Epoch 6/6\n",
      "422/422 - 4s - 9ms/step - accuracy: 0.9729 - loss: 0.0939 - val_accuracy: 0.9750 - val_loss: 0.0861\n",
      "Logs -> C:\\tb_logs\\task4_grid_mlp-20251029-152724__lr-0.0005__hu-256__bs-128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>hidden_units</th>\n",
       "      <th>best_val_accuracy</th>\n",
       "      <th>run_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>0.980667</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152337__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.977833</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152453__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0.977667</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152303__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975833</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152621__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152428__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152724__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152534__lr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>C:\\tb_logs\\task4_grid_mlp-20251029-152659__lr-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr  batch_size  hidden_units  best_val_accuracy  \\\n",
       "0  0.0010          64           256           0.980667   \n",
       "1  0.0010         128           256           0.977833   \n",
       "2  0.0010          64           128           0.977667   \n",
       "3  0.0005          64           256           0.975833   \n",
       "4  0.0010         128           128           0.975500   \n",
       "5  0.0005         128           256           0.975000   \n",
       "6  0.0005          64           128           0.973333   \n",
       "7  0.0005         128           128           0.969000   \n",
       "\n",
       "                                             run_dir  \n",
       "0  C:\\tb_logs\\task4_grid_mlp-20251029-152337__lr-...  \n",
       "1  C:\\tb_logs\\task4_grid_mlp-20251029-152453__lr-...  \n",
       "2  C:\\tb_logs\\task4_grid_mlp-20251029-152303__lr-...  \n",
       "3  C:\\tb_logs\\task4_grid_mlp-20251029-152621__lr-...  \n",
       "4  C:\\tb_logs\\task4_grid_mlp-20251029-152428__lr-...  \n",
       "5  C:\\tb_logs\\task4_grid_mlp-20251029-152724__lr-...  \n",
       "6  C:\\tb_logs\\task4_grid_mlp-20251029-152534__lr-...  \n",
       "7  C:\\tb_logs\\task4_grid_mlp-20251029-152659__lr-...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import itertools, os, pandas as pd, json, shutil\n",
    "\n",
    "def run_grid_search(lrs, bss, hus, epochs=6):\n",
    "    results = []\n",
    "    best = {\"val_accuracy\": -1.0, \"run_dir\": None, \"hparams\": None}\n",
    "    for lr, bs, hu in itertools.product(lrs, bss, hus):\n",
    "        run_dir, _ = train_one_run(lr=lr, hidden_units=hu, batch_size=bs, epochs=epochs, prefix=\"task4_grid_mlp\")\n",
    "        csv_path = os.path.join(run_dir, \"history.csv\")\n",
    "        hist = pd.read_csv(csv_path)\n",
    "        best_val_acc = hist[\"val_accuracy\"].max()\n",
    "        results.append({\"lr\": lr, \"batch_size\": bs, \"hidden_units\": hu, \"best_val_accuracy\": best_val_acc, \"run_dir\": run_dir})\n",
    "        if best_val_acc > best[\"val_accuracy\"]:\n",
    "            best = {\"val_accuracy\": float(best_val_acc), \"run_dir\": run_dir, \"hparams\": {\"lr\": lr, \"batch_size\": bs, \"hidden_units\": hu}}\n",
    "    df = pd.DataFrame(results).sort_values(by=\"best_val_accuracy\", ascending=False).reset_index(drop=True)\n",
    "    return df, best\n",
    "\n",
    "grid_df, best = run_grid_search(lrs=[1e-3, 5e-4], bss=[64, 128], hus=[128, 256], epochs=6)\n",
    "grid_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4a8bf",
   "metadata": {},
   "source": [
    "**Best configuration (this grid):**  \n",
    "- `learning_rate = 1e-3`  \n",
    "- `batch_size = 64`  \n",
    "- `hidden_units = 256`  \n",
    "**Result:** Validation accuracy ≈ **0.9807**\n",
    "\n",
    "**Grid ranges tested:**  \n",
    "- `lr ∈ {1e-3, 5e-4}`  \n",
    "- `batch_size ∈ {64, 128}`  \n",
    "- `hidden_units ∈ {128, 256}`\n",
    "\n",
    "---\n",
    "\n",
    "## What the grid shows\n",
    "\n",
    "### Learning Rate (LR)\n",
    "- **1e-3 > 5e-4** across almost all settings (top 3 runs all use `1e-3`).\n",
    "- **Reasoning:** MNIST is well-conditioned; a slightly larger LR speeds convergence without overshooting. With `5e-4`, curves rise slower and plateau slightly lower.\n",
    "\n",
    "### Batch Size\n",
    "- **64 ≥ 128** by a small, consistent margin at the same LR/width.\n",
    "- **Reasoning:** Smaller batches add gradient noise that can improve generalization (flatter minima). `128` is smoother but a touch lower in final val accuracy.\n",
    "\n",
    "### Hidden Units (Model Width)\n",
    "- **256 > 128** at both LRs and both batch sizes.\n",
    "- **Reasoning:** Wider MLP better captures digit-stroke variation after flattening. No overfitting observed under current epochs/regularization.\n",
    "\n",
    "### Interaction Effects\n",
    "- **LR × Width:** Gains from 256 units are clearest when **LR = 1e-3** (capacity is actually utilized).\n",
    "- **LR × Batch:** With **1e-3**, batch **64** typically edges **128**; with **5e-4**, differences narrow because the step size limits progress.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db339fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save artifacts\n",
    "summary_path = os.path.join(\"artifacts\", \"task4_grid_summary.csv\")\n",
    "grid_df.to_csv(summary_path, index=False)\n",
    "with open(os.path.join(\"artifacts\", \"task4_best.json\"), \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "print(\"Summary saved to:\", summary_path)\n",
    "print(\"Best config:\", best)\n",
    "\n",
    "# Copy best logs to a convenient folder\n",
    "BEST_LOGS_DIR = os.path.join(\"runs\", \"best_run_logs\")\n",
    "if os.path.exists(BEST_LOGS_DIR):\n",
    "    shutil.rmtree(BEST_LOGS_DIR)\n",
    "shutil.copytree(best[\"run_dir\"], BEST_LOGS_DIR)\n",
    "print(\"Best run logs copied to:\", BEST_LOGS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681f16d",
   "metadata": {},
   "source": [
    "## Evaluation Criteria Mapping\n",
    "- TensorBoard Setup: descriptive run names + logs in `runs/`.\n",
    "- Training Metrics: Scalars + custom macro precision/recall/F1.\n",
    "- Model Architecture: Graphs tab (MLP vs CNN) + optional diagrams.\n",
    "- Hyperparameter Tuning: grid, CSV logs, best config JSON.\n",
    "- Code Quality: functions, seeds, checkpoints, early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a65d6",
   "metadata": {},
   "source": [
    "## Summary & Recommendations (fill after running)\n",
    "- Paste key screenshots and write 3–5 bullet recommendations here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
