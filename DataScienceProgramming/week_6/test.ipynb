{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIG =====\n",
    "DATA_CSV = \"radiomics_features_2D_with_AnnotationDetails.csv\"\n",
    "IMG_ROOT = \"/home/24chuong.ta/anaconda3/chun/DICOM 1-100/Image_Encoder_Outputs_Swin\"  # <-- adjust if needed\n",
    "\n",
    "ID_COL     = \"ID\"            # optional, for reporting\n",
    "STEM_COL   = \"__stem__\"      # points to image embedding\n",
    "LABEL_COL  = \"Subtype\"       # target label\n",
    "\n",
    "# Agreed radiomics list (must match CSV headers)\n",
    "SELECTED_RADIOMICS = [\n",
    "    \"Gross size\",\n",
    "    \"her2 expression cd\",\n",
    "    \"original_shape2D_Elongation\",\n",
    "    \"original_shape2D_Perimeter\",\n",
    "    \"original_shape2D_MinorAxisLength\",\n",
    "    \"original_shape2D_MajorAxisLength\",\n",
    "    \"original_shape2D_Sphericity\",\n",
    "    \"original_shape2D_MeshSurface\",\n",
    "    \"original_glrlm_GrayLevelNonUniformity\",\n",
    "    \"original_glrlm_RunLengthNonUniformity\",\n",
    "    \"original_glszm_LargeAreaEmphasis\",\n",
    "    \"original_glszm_LargeAreaLowGrayLevelEmphasis\",\n",
    "    \"original_glszm_ZoneVariance\",\n",
    "    \"original_glszm_ZonePercentage\",\n",
    "    \"original_gldm_DependenceNonUniformity\",\n",
    "    \"original_gldm_DependenceNonUniformityNormalized\",\n",
    "    \"original_gldm_DependenceVariance\",\n",
    "]\n",
    "\n",
    "# Treat these textual columns as categorical for one-hot (add/remove as needed)\n",
    "FORCE_CATEGORICAL = [\n",
    "    \"BI-RAD\", \"Mass Shape\", \"Mass Margin\", \"Diagnosis\",\n",
    "    \"Histologic grade\", \"Calcification morphology\", \"Gross Feature\"\n",
    "]\n",
    "\n",
    "# Labels to drop\n",
    "DROP_LABELS = {\"Unidentifiable\", \"Unknown\", \"\", \"NA\", \"nan\", \"None\", \"N/A\"}\n",
    "\n",
    "TEST_SIZE    = 0.20\n",
    "RANDOM_SEED  = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, normalize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8632239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LOAD & CLEAN =====\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "def clean_label(x):\n",
    "    if pd.isna(x): return \"\"\n",
    "    return str(x).replace(\"\\u00A0\",\" \").replace(\"\\u200B\",\"\").strip()\n",
    "\n",
    "df[LABEL_COL] = df[LABEL_COL].apply(clean_label)\n",
    "before = len(df)\n",
    "df = df[~df[LABEL_COL].isin(DROP_LABELS)].reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} rows by label filter; remaining: {after}\")\n",
    "\n",
    "# Columns present\n",
    "cols = df.columns.tolist()\n",
    "\n",
    "# 1) Radiomics (use only the agreed list if present)\n",
    "radiomics_cols = [c for c in SELECTED_RADIOMICS if c in df.columns]\n",
    "missing_rads   = [c for c in SELECTED_RADIOMICS if c not in df.columns]\n",
    "if missing_rads:\n",
    "    print(\"[warn] Missing radiomics (ignored):\", missing_rads)\n",
    "print(f\"Using {len(radiomics_cols)} radiomics:\", radiomics_cols)\n",
    "\n",
    "# 2) Identify candidates excluding label/ID/stem and the selected radiomics\n",
    "exclude = set([LABEL_COL, ID_COL, STEM_COL]) | set(radiomics_cols)\n",
    "candidate_cols = [c for c in cols if c not in exclude]\n",
    "\n",
    "# We will:\n",
    "# - send object-dtype or forced columns to categorical one-hot\n",
    "# - send numeric columns to 'other numeric' UNLESS they look like extra radiomics:\n",
    "#     any column starting with 'original_' that is NOT in our agreed radiomics list is EXCLUDED\n",
    "forced_set = set(FORCE_CATEGORICAL)\n",
    "cat_cols, num_extra_cols = [], []\n",
    "skipped_extra_original = []\n",
    "\n",
    "for c in candidate_cols:\n",
    "    if c in forced_set:\n",
    "        cat_cols.append(c)\n",
    "        continue\n",
    "\n",
    "    if df[c].dtype == \"O\":  # object/string -> categorical\n",
    "        cat_cols.append(c)\n",
    "        continue\n",
    "\n",
    "    # numeric path\n",
    "    if c.startswith(\"original_\") and (c not in radiomics_cols):\n",
    "        # looks like an extra radiomics feature not in the agreed list -> skip\n",
    "        skipped_extra_original.append(c)\n",
    "        continue\n",
    "\n",
    "    # keep other numeric (non-radiomics) columns\n",
    "    num_extra_cols.append(c)\n",
    "\n",
    "numeric_cols = radiomics_cols + num_extra_cols\n",
    "\n",
    "print(f\"Additional numeric (non-radiomics): {len(num_extra_cols)}\")\n",
    "print(f\"Categorical/text (one-hot): {len(cat_cols)}\")\n",
    "print(f\"Total numeric used: {len(numeric_cols)}\")\n",
    "\n",
    "if skipped_extra_original:\n",
    "    print(f\"Excluded extra 'original_*' columns not in SELECTED_RADIOMICS: {len(skipped_extra_original)}\")\n",
    "    # Uncomment to inspect names:\n",
    "    # print(skipped_extra_original[:20])\n",
    "\n",
    "# Class names (fixed order)\n",
    "class_names = np.unique(df[LABEL_COL].astype(str).values)\n",
    "C = len(class_names)\n",
    "print(\"Classes:\", class_names.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ea480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SPLIT =====\n",
    "y_all = pd.Categorical(df[LABEL_COL].astype(str).values, categories=class_names).codes\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "tr_idx, te_idx = next(sss.split(df, y_all))\n",
    "\n",
    "df_tr = df.iloc[tr_idx].reset_index(drop=True)\n",
    "df_te = df.iloc[te_idx].reset_index(drop=True)\n",
    "\n",
    "y_tr = pd.Categorical(df_tr[LABEL_COL].astype(str).values, categories=class_names).codes\n",
    "y_te = pd.Categorical(df_te[LABEL_COL].astype(str).values, categories=class_names).codes\n",
    "\n",
    "print(f\"Train n={len(df_tr)} | Test n={len(df_te)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f331e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TABULAR PREPROCESSOR =====\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipe, numeric_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "X_tab_tr = pre.fit_transform(df_tr[numeric_cols + cat_cols])\n",
    "X_tab_te = pre.transform(df_te[numeric_cols + cat_cols])\n",
    "\n",
    "feat_names = pre.get_feature_names_out().tolist()\n",
    "print(\"Tabular shapes → train:\", X_tab_tr.shape, \" test:\", X_tab_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMAGE RESOLVER (RAW) =====\n",
    "def stem_to_paths(stem: str):\n",
    "    \"\"\"\n",
    "    e.g., '1062443_0062_LCC' ->\n",
    "    case_dir = '1062443_0062'\n",
    "    cropped  = IMG_ROOT/1062443_0062/1062443_0062_LCC_cropped\n",
    "    \"\"\"\n",
    "    if not isinstance(stem, str) or \"_\" not in stem:\n",
    "        return None, None\n",
    "    parts = stem.split(\"_\")\n",
    "    if len(parts) < 3:\n",
    "        return None, None\n",
    "    case_dir = \"_\".join(parts[:2])\n",
    "    view     = parts[2]\n",
    "    cropped_dir = os.path.join(IMG_ROOT, case_dir, f\"{case_dir}_{view}_cropped\")\n",
    "    return case_dir, cropped_dir\n",
    "\n",
    "def find_emb_file(cropped_dir: str):\n",
    "    if not cropped_dir or not os.path.isdir(cropped_dir): return None\n",
    "    p = Path(cropped_dir)\n",
    "    # prefer npy\n",
    "    npys = sorted(glob.glob(str(p / \"*crop_emb.npy\")))\n",
    "    if npys: return npys[0]\n",
    "    csvs = sorted(glob.glob(str(p / \"*crop_emb.csv\")) + glob.glob(str(p / \"*crop_emb.CSV\")))\n",
    "    if csvs: return csvs[0]\n",
    "    return None\n",
    "\n",
    "def load_embedding(fpath: str):\n",
    "    if fpath is None: return None\n",
    "    if fpath.lower().endswith(\".npy\"):\n",
    "        v = np.load(fpath).astype(float).reshape(-1)\n",
    "    else:\n",
    "        v = pd.read_csv(fpath, header=None).to_numpy().astype(float).reshape(-1)\n",
    "    return v\n",
    "\n",
    "def build_image_matrix(df_split):\n",
    "    vecs, has_img = [], []\n",
    "    files = []\n",
    "    for stem in df_split[STEM_COL].astype(str).tolist():\n",
    "        _, cropped = stem_to_paths(stem)\n",
    "        f = find_emb_file(cropped)\n",
    "        files.append(f)\n",
    "        if f is None:\n",
    "            vecs.append(None); has_img.append(0.0)\n",
    "        else:\n",
    "            v = load_embedding(f)\n",
    "            vecs.append(v);   has_img.append(1.0)\n",
    "    # common width by max length\n",
    "    max_dim = max((len(v) for v in vecs if isinstance(v, np.ndarray)), default=0)\n",
    "    X_raw = np.zeros((len(df_split), max_dim), dtype=float)\n",
    "    for i, v in enumerate(vecs):\n",
    "        if isinstance(v, np.ndarray):\n",
    "            d = min(max_dim, len(v))\n",
    "            X_raw[i, :d] = v[:d]\n",
    "    # row-wise L2 norm (safe; no leakage)\n",
    "    X_raw = normalize(X_raw, norm=\"l2\", axis=1) if max_dim > 0 else X_raw\n",
    "    has_flag = np.array(has_img, dtype=np.float32).reshape(-1, 1)\n",
    "    return X_raw, has_flag, files\n",
    "\n",
    "X_img_raw_tr, has_img_tr, files_tr = build_image_matrix(df_tr)\n",
    "X_img_raw_te, has_img_te, files_te = build_image_matrix(df_te)\n",
    "\n",
    "print(\"Image shapes → train:\", X_img_raw_tr.shape, \" test:\", X_img_raw_te.shape)\n",
    "print(\"Image coverage → train:\", float(has_img_tr.mean()), \" test:\", float(has_img_te.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb50808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FUSE =====\n",
    "def fuse(tab, img_raw, has_flag):\n",
    "    parts = [tab]\n",
    "    if img_raw is not None and img_raw.shape[1] > 0: parts.append(img_raw)\n",
    "    if has_flag is not None: parts.append(has_flag)\n",
    "    return np.hstack(parts)\n",
    "\n",
    "X_tr = fuse(X_tab_tr, X_img_raw_tr, has_img_tr)\n",
    "X_te = fuse(X_tab_te, X_img_raw_te, has_img_te)\n",
    "\n",
    "print(\"Fused dims → train:\", X_tr.shape, \" test:\", X_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35923c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== RANDOM FOREST =====\n",
    "weights = compute_class_weight(\"balanced\", classes=np.arange(C), y=y_tr)\n",
    "class_weight = {i: w for i, w in enumerate(weights)}\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=800,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    max_features=\"sqrt\",\n",
    "    class_weight=class_weight,\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "rf.fit(X_tr, y_tr)\n",
    "proba_te = rf.predict_proba(X_te)\n",
    "\n",
    "# Align to full class set (if any class missing in train)\n",
    "proba_full = np.zeros((len(y_te), C), dtype=float)\n",
    "proba_full[:, rf.classes_] = proba_te\n",
    "y_pred = proba_full.argmax(1)\n",
    "\n",
    "acc  = accuracy_score(y_te, y_pred)\n",
    "f1m  = f1_score(y_te, y_pred, average=\"macro\")\n",
    "f1u  = f1_score(y_te, y_pred, average=\"micro\")\n",
    "f1w  = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "print(f\"[Tabular + Image(raw)] Acc={acc:.3f} | F1(macro)={f1m:.3f} | F1(micro)={f1u:.3f} | F1(weighted)={f1w:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    y_te, y_pred,\n",
    "    labels=np.arange(C), target_names=class_names,\n",
    "    digits=3, zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c100b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFUSION MATRIX =====\n",
    "def show_confusion_matrix(y_true, y_hat, class_names, title=\"Confusion Matrix\", normalize=False):\n",
    "    cm = confusion_matrix(y_true, y_hat, labels=np.arange(len(class_names)))\n",
    "    disp = cm.astype(float) / cm.sum(axis=1, keepdims=True).clip(min=1) if normalize else cm\n",
    "    print(f\"{title} (rows=true, cols=pred):\\n\", cm)\n",
    "    fig, ax = plt.subplots(figsize=(6,5), dpi=140)\n",
    "    ax.imshow(disp, interpolation=\"nearest\")\n",
    "    ax.set_xticks(np.arange(len(class_names))); ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(len(class_names))); ax.set_yticklabels(class_names)\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title + (\" (normalized)\" if normalize else \"\"))\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = f\"{disp[i,j]:.2f}\" if normalize else f\"{cm[i,j]}\"\n",
    "            ax.text(j, i, val, ha=\"center\", va=\"center\", fontsize=8)\n",
    "    plt.tight_layout(); plt.show()\n",
    "    return cm\n",
    "\n",
    "_ = show_confusion_matrix(y_te, y_pred, class_names, title=\"Confusion Matrix — Tabular + Image(raw)\", normalize=False)\n",
    "\n",
    "# ===== PER-CLASS PR CURVES =====\n",
    "Y_bin = pd.get_dummies(pd.Series(y_te), columns=range(C)).values\n",
    "ap_per_class = {}\n",
    "fig, ax = plt.subplots(figsize=(7,5), dpi=140)\n",
    "for k, name in enumerate(class_names):\n",
    "    precision, recall, _ = precision_recall_curve(Y_bin[:, k], proba_full[:, k])\n",
    "    ap = average_precision_score(Y_bin[:, k], proba_full[:, k])\n",
    "    ap_per_class[name] = ap\n",
    "    ax.plot(recall, precision, label=f\"{name} (AP={ap:.2f})\")\n",
    "ax.set_xlabel(\"Recall\"); ax.set_ylabel(\"Precision\"); ax.grid(True, alpha=0.3)\n",
    "ax.set_title(\"Per-class PR — Tabular + Image(raw)\")\n",
    "ax.legend(fontsize=8); plt.tight_layout(); plt.show()\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"variant\": \"tabular_plus_image_raw\",\n",
    "    \"n_features\": X_tr.shape[1],\n",
    "    \"acc\": acc, \"f1_macro\": f1m, \"f1_micro\": f1u, \"f1_weighted\": f1w,\n",
    "    **{f\"AP_{k}\": v for k, v in ap_per_class.items()}\n",
    "}])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
